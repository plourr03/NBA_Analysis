{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1882049a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.tsl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_scorer, f1_score\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeaveOneOut, cross_val_score\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, LeakyReLU, BatchNormalization\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\src\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\keras\\src\\engine\\functional.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\tensorflow\\core\\protobuf\\config_pb2.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_debug__pb2\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_rewriter__config__pb2\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rpc_options_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_rpc__options__pb2\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   tensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2 \u001b[38;5;241m=\u001b[39m tensorflow_dot_core_dot_protobuf_dot_rpc__options__pb2\u001b[38;5;241m.\u001b[39mtensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\tensorflow\\core\\protobuf\\rpc_options_pb2.py:14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rpc_options_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc_options_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor_pool\u001b[38;5;241m.\u001b[39mDefault()\u001b[38;5;241m.\u001b[39mAddSerializedFile(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m*tensorflow/core/protobuf/rpc_options.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mtensorflow.dummy\u001b[39m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m)tensorflow/tsl/protobuf/rpc_options.protoBWZUgithub.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_protoP\u001b[39m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.tsl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import warnings\n",
    "from utils2 import PyroBayesianLogisticRegression, k_fold_cross_validation\n",
    "from joblib import Parallel, delayed\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5554ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(player_id, pts_thresh):\n",
    "    server = 'localhost\\SQLEXPRESS'\n",
    "    database = 'nba_game_data'\n",
    "\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # Define the stored procedure call\n",
    "    stored_proc_call = \"{CALL seasonPtsDataSet (?, ?)}\"  # Replace with your stored procedure name\n",
    "\n",
    "    # Execute the stored procedure\n",
    "    cursor.execute(stored_proc_call, (pts_thresh, player_id))\n",
    "\n",
    "    # Fetch the data\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    # Execute the stored procedure\n",
    "    cursor.execute(stored_proc_call, (pts_thresh, player_id))\n",
    "\n",
    "    # Fetch the data\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    # Get column names from cursor description\n",
    "    columns = [column[0] for column in cursor.description]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame.from_records(data, columns=columns)\n",
    "\n",
    "    # Return Data\n",
    "    return df.iloc[:-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(X,pred):    \n",
    "    # Number of features in the DataFrame\n",
    "    N = X.shape[1]\n",
    "\n",
    "    # Target number of dimensions\n",
    "    M = round(len(X)*.15) \n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_season_scaled = scaler.fit_transform(X)\n",
    "    pred_scaled = scaler.fit_transform(pred)\n",
    "\n",
    "    # Define the model architecture\n",
    "    input_layer = Input(shape=(N,))\n",
    "    # Encoder layer, with BatchNormalization and LeakyReLU\n",
    "    encoded = Dense(M)(input_layer)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = LeakyReLU(alpha=0.01)(encoded)  \n",
    "    # Decoder layer, output size is N (to reconstruct the original data)\n",
    "    decoded = Dense(N, activation='sigmoid')(encoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(df_season_scaled, df_season_scaled,\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=False)\n",
    "\n",
    "    # Extract the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    # Encode the scaled data\n",
    "    df_season_encoded = encoder.predict(df_season_scaled,verbose = False)\n",
    "\n",
    "    # Create a new DataFrame for the encoded data\n",
    "    df_season_reduced = pd.DataFrame(df_season_encoded, columns=[f'Feature{i+1}' for i in range(M)])\n",
    "\n",
    "    # Debugging: Inspect Intermediate Outputs to check for all-zero columns\n",
    "#     encoder_output_model = Model(input_layer, encoded)\n",
    "#     encoder_outputs = encoder_output_model.predict(df_season_scaled, verbose = True)\n",
    "    \n",
    "    pred = encoder.predict(pred_scaled, verbose = False)\n",
    "    \n",
    "    return df_season_reduced, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e7e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2da672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(1628374,24.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['PtsThresh'])\n",
    "y = df['PtsThresh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c087f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en, junk = encode_features(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86276b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en['PtsThresh'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_ = int(len(X_en)*.5)\n",
    "folds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment and specify the number of folds for cross-validation\n",
    "exp_clf = setup(data=X_en, target='PtsThresh', session_id=1234, fold=15)  # Increase folds here\n",
    "\n",
    "# Compare models to find the best one based on default metrics\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1664cdb3-4558-4c2a-b10a-631727862b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "\n'tune_sklearn' is a soft dependency and not included in the pycaret installation. Please run: `pip install tune-sklearn ray[tune]` to install.\nAlternately, you can install this by running `pip install pycaret[tuners]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tune the best model using TPE optimization with Optuna\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_library\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtune-sklearn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoose_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Display the tuned best model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model)\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globals_d[name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\pycaret\\classification\\functional.py:1208\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;129m@check_if_global_is_not_none\u001b[39m(\u001b[38;5;28mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1019\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1038\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \n\u001b[0;32m   1206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_CURRENT_EXPERIMENT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_scorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_library\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_library\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_algorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_max_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_max_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoose_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoose_better\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tuner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tuner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuner_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuner_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\pycaret\\classification\\oop.py:1558\u001b[0m, in \u001b[0;36mClassificationExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1369\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_scorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_library\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_library\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_algorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_max_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_max_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoose_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoose_better\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tuner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tuner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuner_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuner_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2070\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2066\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscikit-optimize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m search_algorithm parameter must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(possible_search_algorithms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m search_library \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune-sklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[43m_check_soft_dependencies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtune_sklearn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtuners\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseverity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstall_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtune-sklearn ray[tune]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m search_algorithm:\n\u001b[0;32m   2078\u001b[0m         search_algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\T-drive\\Jupyter\\envs\\flask\\lib\\site-packages\\pycaret\\utils\\_dependencies.py:152\u001b[0m, in \u001b[0;36m_check_soft_dependencies\u001b[1;34m(package, severity, extra, install_name)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m severity \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m severity \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: \n'tune_sklearn' is a soft dependency and not included in the pycaret installation. Please run: `pip install tune-sklearn ray[tune]` to install.\nAlternately, you can install this by running `pip install pycaret[tuners]`"
     ]
    }
   ],
   "source": [
    "# Tune the best model using TPE optimization with Optuna\n",
    "best_model = tune_model(best_model, search_algorithm='bayesian', n_iter=50, search_library='sklean', choose_better=True)\n",
    "# Display the tuned best model\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623cea4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12992\\3028134865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# X_en.drop(columns=['PtsThresh'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PtsThresh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "X_en.drop(columns=['PtsThresh'])\n",
    "predictions = predict_model(model, data=X_en.drop(columns=['PtsThresh']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "276e462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Feature3</th>\n",
       "      <th>Feature4</th>\n",
       "      <th>Feature5</th>\n",
       "      <th>Feature6</th>\n",
       "      <th>Feature7</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270759</td>\n",
       "      <td>0.996973</td>\n",
       "      <td>0.320384</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.004252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.682985</td>\n",
       "      <td>0.832869</td>\n",
       "      <td>0.248745</td>\n",
       "      <td>1.463021</td>\n",
       "      <td>-0.006382</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851928</td>\n",
       "      <td>1.070348</td>\n",
       "      <td>0.440286</td>\n",
       "      <td>1.143659</td>\n",
       "      <td>-0.002454</td>\n",
       "      <td>-0.004107</td>\n",
       "      <td>-0.007365</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.343338</td>\n",
       "      <td>1.010278</td>\n",
       "      <td>1.055181</td>\n",
       "      <td>1.035669</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.013013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.449142</td>\n",
       "      <td>1.073285</td>\n",
       "      <td>0.488032</td>\n",
       "      <td>0.583266</td>\n",
       "      <td>0.167180</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>-0.006897</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.709851</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.467653</td>\n",
       "      <td>0.376221</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>-0.004610</td>\n",
       "      <td>-0.004961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.354999</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>0.332401</td>\n",
       "      <td>0.514578</td>\n",
       "      <td>-0.003310</td>\n",
       "      <td>-0.006434</td>\n",
       "      <td>-0.004368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.912087</td>\n",
       "      <td>0.518471</td>\n",
       "      <td>1.051061</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>-0.002768</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.620342</td>\n",
       "      <td>0.758724</td>\n",
       "      <td>1.444981</td>\n",
       "      <td>0.457451</td>\n",
       "      <td>0.112311</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.577709</td>\n",
       "      <td>0.042392</td>\n",
       "      <td>1.822886</td>\n",
       "      <td>1.132611</td>\n",
       "      <td>0.201942</td>\n",
       "      <td>-0.001958</td>\n",
       "      <td>1.409826</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.754833</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>1.416703</td>\n",
       "      <td>0.496745</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>-0.001827</td>\n",
       "      <td>1.209811</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.438670</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>1.440010</td>\n",
       "      <td>0.411266</td>\n",
       "      <td>-0.009182</td>\n",
       "      <td>-0.010183</td>\n",
       "      <td>0.626314</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.702287</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>1.503320</td>\n",
       "      <td>0.534602</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>-0.012512</td>\n",
       "      <td>0.723662</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.369234</td>\n",
       "      <td>0.038742</td>\n",
       "      <td>1.677279</td>\n",
       "      <td>0.447864</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.011632</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.870603</td>\n",
       "      <td>0.635912</td>\n",
       "      <td>1.555447</td>\n",
       "      <td>1.099131</td>\n",
       "      <td>-0.009132</td>\n",
       "      <td>-0.010171</td>\n",
       "      <td>-0.002081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.607485</td>\n",
       "      <td>0.096787</td>\n",
       "      <td>1.875083</td>\n",
       "      <td>0.516884</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>-0.008688</td>\n",
       "      <td>-0.002492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.567550</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>1.488104</td>\n",
       "      <td>1.209718</td>\n",
       "      <td>-0.006960</td>\n",
       "      <td>-0.014536</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.908367</td>\n",
       "      <td>0.808630</td>\n",
       "      <td>1.452116</td>\n",
       "      <td>0.417955</td>\n",
       "      <td>-0.003622</td>\n",
       "      <td>-0.004400</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.310930</td>\n",
       "      <td>0.648715</td>\n",
       "      <td>0.460063</td>\n",
       "      <td>0.613624</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>0.512474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.427998</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.500378</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>-0.007658</td>\n",
       "      <td>0.511747</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.796888</td>\n",
       "      <td>0.648035</td>\n",
       "      <td>0.346040</td>\n",
       "      <td>1.154105</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>-0.009777</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.598552</td>\n",
       "      <td>0.089805</td>\n",
       "      <td>0.712694</td>\n",
       "      <td>0.526878</td>\n",
       "      <td>-0.012460</td>\n",
       "      <td>-0.012504</td>\n",
       "      <td>1.595615</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.926326</td>\n",
       "      <td>0.056759</td>\n",
       "      <td>1.482783</td>\n",
       "      <td>0.553219</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.013831</td>\n",
       "      <td>0.923933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.548456</td>\n",
       "      <td>0.730398</td>\n",
       "      <td>1.348923</td>\n",
       "      <td>1.127937</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>-0.011097</td>\n",
       "      <td>-0.002435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.783187</td>\n",
       "      <td>0.870557</td>\n",
       "      <td>1.580347</td>\n",
       "      <td>0.614250</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>-0.010011</td>\n",
       "      <td>-0.006350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.367485</td>\n",
       "      <td>-0.000575</td>\n",
       "      <td>1.377687</td>\n",
       "      <td>1.313852</td>\n",
       "      <td>-0.004746</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>1.163056</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.827460</td>\n",
       "      <td>0.145060</td>\n",
       "      <td>0.515597</td>\n",
       "      <td>1.316575</td>\n",
       "      <td>-0.001799</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>0.689403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.822512</td>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.417622</td>\n",
       "      <td>0.574491</td>\n",
       "      <td>-0.007114</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>0.318425</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.434020</td>\n",
       "      <td>0.661412</td>\n",
       "      <td>0.088196</td>\n",
       "      <td>1.320224</td>\n",
       "      <td>-0.002947</td>\n",
       "      <td>-0.003348</td>\n",
       "      <td>1.158507</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.047355</td>\n",
       "      <td>0.569145</td>\n",
       "      <td>1.408063</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>1.129416</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.312859</td>\n",
       "      <td>0.029902</td>\n",
       "      <td>0.472436</td>\n",
       "      <td>1.302714</td>\n",
       "      <td>-0.008010</td>\n",
       "      <td>-0.007155</td>\n",
       "      <td>1.101114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.294417</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>0.194913</td>\n",
       "      <td>1.144790</td>\n",
       "      <td>-0.005069</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>0.919659</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.395674</td>\n",
       "      <td>0.518059</td>\n",
       "      <td>0.670282</td>\n",
       "      <td>1.330711</td>\n",
       "      <td>-0.004446</td>\n",
       "      <td>-0.011842</td>\n",
       "      <td>-0.005055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.543766</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>0.294092</td>\n",
       "      <td>1.345176</td>\n",
       "      <td>-0.012362</td>\n",
       "      <td>-0.010333</td>\n",
       "      <td>0.372374</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.778196</td>\n",
       "      <td>0.014958</td>\n",
       "      <td>1.012786</td>\n",
       "      <td>1.304625</td>\n",
       "      <td>-0.010160</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>0.155408</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.507325</td>\n",
       "      <td>0.665249</td>\n",
       "      <td>1.025770</td>\n",
       "      <td>1.324359</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>-0.000943</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.536446</td>\n",
       "      <td>1.034145</td>\n",
       "      <td>0.862098</td>\n",
       "      <td>1.483117</td>\n",
       "      <td>0.047314</td>\n",
       "      <td>-0.003542</td>\n",
       "      <td>0.082297</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.189303</td>\n",
       "      <td>0.134132</td>\n",
       "      <td>0.831415</td>\n",
       "      <td>1.326443</td>\n",
       "      <td>-0.010918</td>\n",
       "      <td>-0.008036</td>\n",
       "      <td>0.370390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.237562</td>\n",
       "      <td>-0.006052</td>\n",
       "      <td>0.855317</td>\n",
       "      <td>1.847264</td>\n",
       "      <td>0.317554</td>\n",
       "      <td>-0.010976</td>\n",
       "      <td>0.734632</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.227200</td>\n",
       "      <td>-0.002077</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.609232</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>-0.010123</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.680991</td>\n",
       "      <td>-0.003162</td>\n",
       "      <td>1.095187</td>\n",
       "      <td>1.462482</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>-0.001557</td>\n",
       "      <td>0.124935</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.209463</td>\n",
       "      <td>0.322135</td>\n",
       "      <td>1.409306</td>\n",
       "      <td>1.655413</td>\n",
       "      <td>-0.003518</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>0.696646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.612290</td>\n",
       "      <td>0.858877</td>\n",
       "      <td>0.416817</td>\n",
       "      <td>0.667737</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.002468</td>\n",
       "      <td>-0.005778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.963281</td>\n",
       "      <td>0.674097</td>\n",
       "      <td>0.865301</td>\n",
       "      <td>0.797632</td>\n",
       "      <td>-0.006564</td>\n",
       "      <td>0.141477</td>\n",
       "      <td>-0.004948</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.432592</td>\n",
       "      <td>0.930088</td>\n",
       "      <td>0.215937</td>\n",
       "      <td>0.812072</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.004488</td>\n",
       "      <td>0.029227</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature1  Feature2  Feature3  Feature4  Feature5  Feature6  Feature7  \\\n",
       "0   0.270759  0.996973  0.320384  0.520368 -0.002515 -0.002754 -0.004252   \n",
       "1   0.682985  0.832869  0.248745  1.463021 -0.006382 -0.009701 -0.000268   \n",
       "2   0.851928  1.070348  0.440286  1.143659 -0.002454 -0.004107 -0.007365   \n",
       "3   1.343338  1.010278  1.055181  1.035669 -0.000070 -0.000720 -0.013013   \n",
       "4   1.449142  1.073285  0.488032  0.583266  0.167180 -0.001447 -0.006897   \n",
       "5   0.709851  0.616822  0.467653  0.376221 -0.001082 -0.004610 -0.004961   \n",
       "6   0.354999 -0.000340  0.332401  0.514578 -0.003310 -0.006434 -0.004368   \n",
       "7   0.626483  0.912087  0.518471  1.051061  0.032742 -0.002768 -0.002774   \n",
       "8   1.620342  0.758724  1.444981  0.457451  0.112311 -0.001036 -0.002590   \n",
       "9   0.577709  0.042392  1.822886  1.132611  0.201942 -0.001958  1.409826   \n",
       "10  0.754833 -0.000451  1.416703  0.496745  0.008891 -0.001827  1.209811   \n",
       "11  0.438670  0.005937  1.440010  0.411266 -0.009182 -0.010183  0.626314   \n",
       "12  0.702287  0.016632  1.503320  0.534602 -0.011190 -0.012512  0.723662   \n",
       "13  2.369234  0.038742  1.677279  0.447864 -0.002210 -0.011632 -0.003863   \n",
       "14  1.870603  0.635912  1.555447  1.099131 -0.009132 -0.010171 -0.002081   \n",
       "15  1.607485  0.096787  1.875083  0.516884 -0.001496 -0.008688 -0.002492   \n",
       "16  1.567550 -0.000328  1.488104  1.209718 -0.006960 -0.014536 -0.001641   \n",
       "17  0.908367  0.808630  1.452116  0.417955 -0.003622 -0.004400 -0.002848   \n",
       "18  1.310930  0.648715  0.460063  0.613624 -0.007692 -0.001495  0.512474   \n",
       "19  1.427998  0.697900  0.500378  0.417582 -0.001618 -0.007658  0.511747   \n",
       "20  1.796888  0.648035  0.346040  1.154105 -0.000593 -0.009777 -0.002262   \n",
       "21  0.598552  0.089805  0.712694  0.526878 -0.012460 -0.012504  1.595615   \n",
       "22  0.926326  0.056759  1.482783  0.553219 -0.009752 -0.013831  0.923933   \n",
       "23  1.548456  0.730398  1.348923  1.127937 -0.009218 -0.011097 -0.002435   \n",
       "24  1.783187  0.870557  1.580347  0.614250 -0.008655 -0.010011 -0.006350   \n",
       "25  1.367485 -0.000575  1.377687  1.313852 -0.004746 -0.005391  1.163056   \n",
       "26  0.827460  0.145060  0.515597  1.316575 -0.001799 -0.001346  0.689403   \n",
       "27  0.822512  0.626483  0.417622  0.574491 -0.007114 -0.000881  0.318425   \n",
       "28  0.434020  0.661412  0.088196  1.320224 -0.002947 -0.003348  1.158507   \n",
       "29  0.921002  0.047355  0.569145  1.408063 -0.009004 -0.001896  1.129416   \n",
       "30  0.312859  0.029902  0.472436  1.302714 -0.008010 -0.007155  1.101114   \n",
       "31  0.294417 -0.001010  0.194913  1.144790 -0.005069 -0.008895  0.919659   \n",
       "32  1.395674  0.518059  0.670282  1.330711 -0.004446 -0.011842 -0.005055   \n",
       "33  0.543766  0.066101  0.294092  1.345176 -0.012362 -0.010333  0.372374   \n",
       "34  0.778196  0.014958  1.012786  1.304625 -0.010160 -0.002848  0.155408   \n",
       "35  0.507325  0.665249  1.025770  1.324359 -0.001713 -0.002685 -0.000943   \n",
       "36  0.536446  1.034145  0.862098  1.483117  0.047314 -0.003542  0.082297   \n",
       "37  0.189303  0.134132  0.831415  1.326443 -0.010918 -0.008036  0.370390   \n",
       "38  0.237562 -0.006052  0.855317  1.847264  0.317554 -0.010976  0.734632   \n",
       "39  0.227200 -0.002077  0.224506  0.609232 -0.003928 -0.010123 -0.002643   \n",
       "40  0.680991 -0.003162  1.095187  1.462482 -0.002151 -0.001557  0.124935   \n",
       "41  0.209463  0.322135  1.409306  1.655413 -0.003518 -0.006986  0.696646   \n",
       "42  0.612290  0.858877  0.416817  0.667737 -0.001382 -0.002468 -0.005778   \n",
       "43  0.963281  0.674097  0.865301  0.797632 -0.006564  0.141477 -0.004948   \n",
       "44  1.432592  0.930088  0.215937  0.812072 -0.001418 -0.004488  0.029227   \n",
       "\n",
       "    prediction_label  prediction_score  actual  \n",
       "0                  0            0.9345       0  \n",
       "1                  0            0.9642       0  \n",
       "2                  0            0.9596       0  \n",
       "3                  0            0.9564       0  \n",
       "4                  1            1.0000       1  \n",
       "5                  0            0.9254       0  \n",
       "6                  0            0.9470       1  \n",
       "7                  1            0.9895       0  \n",
       "8                  1            1.0000       0  \n",
       "9                  1            1.0000       1  \n",
       "10                 0            0.7910       1  \n",
       "11                 0            0.9584       0  \n",
       "12                 0            0.9585       0  \n",
       "13                 0            0.8642       1  \n",
       "14                 0            0.9518       1  \n",
       "15                 0            0.9678       1  \n",
       "16                 0            0.9753       0  \n",
       "17                 0            0.9609       1  \n",
       "18                 0            0.9191       0  \n",
       "19                 0            0.8867       1  \n",
       "20                 0            0.9137       0  \n",
       "21                 0            0.5649       1  \n",
       "22                 0            0.9467       0  \n",
       "23                 0            0.9606       0  \n",
       "24                 0            0.9356       0  \n",
       "25                 0            0.9307       1  \n",
       "26                 0            0.9620       1  \n",
       "27                 0            0.9441       1  \n",
       "28                 0            0.9042       0  \n",
       "29                 0            0.9097       0  \n",
       "30                 0            0.9217       0  \n",
       "31                 0            0.9446       0  \n",
       "32                 0            0.9583       0  \n",
       "33                 0            0.9584       1  \n",
       "34                 0            0.9742       0  \n",
       "35                 0            0.9762       0  \n",
       "36                 1            1.0000       1  \n",
       "37                 0            0.9678       0  \n",
       "38                 1            1.0000       1  \n",
       "39                 0            0.9526       0  \n",
       "40                 0            0.9809       0  \n",
       "41                 0            0.9779       0  \n",
       "42                 0            0.9480       1  \n",
       "43                 0            1.0000       0  \n",
       "44                 0            0.9244       1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict_model(best_model, data=X_en.drop(columns=['PtsThresh']))\n",
    "predictions['actual'] = y\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3159c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=3))\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X_en, y, cv=12, scoring='f1', n_jobs=-1)\n",
    "    results.append((name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54de7442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pyro', 0.3444444444444445)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best model based on mean F1 score, break ties with standard deviation\n",
    "best_model_name, best_score, _ = max(results, key=lambda x: (x[1], -x[2]))\n",
    "\n",
    "best_model_name, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fc7b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:35<00:00,  5.66trial/s, best loss: -0.37777777777777777]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', max_depth=12,\n",
       "                       max_features='auto', min_samples_leaf=15,\n",
       "                       min_samples_split=9,\n",
       "                       min_weight_fraction_leaf=0.051268956971994385,\n",
       "                       splitter='random')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preform_dt_parameter_tuning(X,y):\n",
    "    # Expanded search space with additional parameters\n",
    "    space = {\n",
    "        'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "        'splitter': hp.choice('splitter', ['best', 'random']),\n",
    "        'max_depth': hp.choice('max_depth', range(1, 15)),\n",
    "        'min_samples_split': hp.uniform('min_samples_split', 0.01, 0.2),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0.01, 0.2),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "        'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0, 0.5),\n",
    "        'max_leaf_nodes': hp.choice('max_leaf_nodes', [None] + list(range(2, 100)))\n",
    "    }\n",
    "\n",
    "    # Objective function with LOO cross-validation\n",
    "    def objective(space):\n",
    "        classifier = DecisionTreeClassifier(\n",
    "            criterion=space['criterion'],\n",
    "            splitter=space['splitter'],\n",
    "            max_depth=space['max_depth'],\n",
    "            min_samples_split=int(space['min_samples_split'] * 100),\n",
    "            min_samples_leaf=int(space['min_samples_leaf'] * 100),\n",
    "            max_features=space['max_features'],\n",
    "            class_weight=space['class_weight'],\n",
    "            min_weight_fraction_leaf=space['min_weight_fraction_leaf'],\n",
    "            max_leaf_nodes=space['max_leaf_nodes']\n",
    "        )\n",
    "\n",
    "        # Using Leave-One-Out cross-validation\n",
    "        loo = LeaveOneOut()\n",
    "        accuracy = cross_val_score(classifier, X, y, cv=loo, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "        # Return the loss and status\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "    # Run the optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=200,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Assuming `best` is the dictionary of best parameters returned by `hyperopt`\n",
    "    # Map `hp.choice` indices to their corresponding values for all parameters using `hp.choice`\n",
    "    criterion_options = ['gini', 'entropy']\n",
    "    splitter_options = ['best', 'random']\n",
    "    max_features_options = ['auto', 'sqrt', 'log2', None]\n",
    "    class_weight_options = [None, 'balanced']\n",
    "\n",
    "    best_criterion = criterion_options[best['criterion']]\n",
    "    best_splitter = splitter_options[best['splitter']]\n",
    "    best_max_features = max_features_options[best['max_features']]\n",
    "    best_class_weight = class_weight_options[best['class_weight']]\n",
    "    # For max_leaf_nodes, handle None as a separate case\n",
    "    max_leaf_nodes_options = [None] + list(range(2, 100))\n",
    "    best_max_leaf_nodes = max_leaf_nodes_options[best['max_leaf_nodes']]\n",
    "\n",
    "    # For parameters not using `hp.choice`, you can use them directly\n",
    "    best_max_depth = best['max_depth']\n",
    "    best_min_samples_split = int(best['min_samples_split'] * 100)\n",
    "    best_min_samples_leaf = int(best['min_samples_leaf'] * 100)\n",
    "    best_min_weight_fraction_leaf = best['min_weight_fraction_leaf']\n",
    "\n",
    "    # Train the Decision Tree classifier with the best parameters\n",
    "    classifier = DecisionTreeClassifier(\n",
    "        criterion=best_criterion,\n",
    "        splitter=best_splitter,\n",
    "        max_depth=best_max_depth,\n",
    "        min_samples_split=best_min_samples_split,\n",
    "        min_samples_leaf=best_min_samples_leaf,\n",
    "        max_features=best_max_features,\n",
    "        class_weight=best_class_weight,\n",
    "        min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    "        max_leaf_nodes=best_max_leaf_nodes\n",
    "    )\n",
    "\n",
    "    # Assuming X and y are your data and labels, and X has been encoded as X_en if necessary\n",
    "    classifier.fit(X, y)  # Replace X with X_en if your dataset is encoded\n",
    "    return classifier\n",
    "\n",
    "preform_dt_parameter_tuning(X_en,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ef1efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:12<00:00,  8.05trial/s, best loss: -0.6222222222222222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', leaf_size=26, n_neighbors=14)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perform_knn_parameter_tuning(X, y):\n",
    "    # Define the search space for KNN parameters\n",
    "    space = {\n",
    "        'n_neighbors': hp.choice('n_neighbors', range(1, 31)),  # Number of neighbors to use\n",
    "        'weights': hp.choice('weights', ['uniform', 'distance']),  # Weight function used in prediction\n",
    "        'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),  # Algorithm to compute nearest neighbors\n",
    "        'leaf_size': hp.choice('leaf_size', range(1, 50)),  # Leaf size passed to BallTree or KDTree\n",
    "        'p': hp.choice('p', [1, 2]),  # Power parameter for the Minkowski metric\n",
    "    }\n",
    "\n",
    "    # Objective function with LOO cross-validation\n",
    "    def objective(space):\n",
    "        classifier = KNeighborsClassifier(\n",
    "            n_neighbors=space['n_neighbors'],\n",
    "            weights=space['weights'],\n",
    "            algorithm=space['algorithm'],\n",
    "            leaf_size=space['leaf_size'],\n",
    "            p=space['p']\n",
    "        )\n",
    "\n",
    "        # Using Leave-One-Out cross-validation\n",
    "        loo = LeaveOneOut()\n",
    "        accuracy = cross_val_score(classifier, X, y, cv=loo, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "        # Return the loss and status\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "    # Run the optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=100,  # Adjust the number of evaluations based on computational budget\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Map `hp.choice` indices to their corresponding values\n",
    "    weights_options = ['uniform', 'distance']\n",
    "    algorithm_options = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    p_options = [1, 2]\n",
    "\n",
    "    best_weights = weights_options[best['weights']]\n",
    "    best_algorithm = algorithm_options[best['algorithm']]\n",
    "    best_p = p_options[best['p']]\n",
    "    \n",
    "    # For parameters using `hp.choice`, map indices to values\n",
    "    best_n_neighbors = best['n_neighbors'] + 1  # `range` starts at 1 for `n_neighbors`\n",
    "    best_leaf_size = best['leaf_size'] + 1  # `range` starts at 1 for `leaf_size`\n",
    "\n",
    "    # Instantiate the KNN classifier with the best parameters\n",
    "    classifier = KNeighborsClassifier(\n",
    "        n_neighbors=best_n_neighbors,\n",
    "        weights=best_weights,\n",
    "        algorithm=best_algorithm,\n",
    "        leaf_size=best_leaf_size,\n",
    "        p=best_p\n",
    "    )\n",
    "\n",
    "    # Train the KNN classifier with the best parameters\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    "perform_knn_parameter_tuning(X_en,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c592f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:11<00:00,  9.03trial/s, best loss: -0.6222222222222222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.012821684098058778, max_iter=1000, penalty='l1',\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perform_lr_parameter_tuning(X, y):\n",
    "    # Define the search space for logistic regression\n",
    "    space = {\n",
    "        'C': hp.loguniform('C', np.log(0.001), np.log(1000)),  # Regularization strength\n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet', 'none']),  # Regularization type\n",
    "        'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),  # Algorithm to use\n",
    "    }\n",
    "\n",
    "    # Make sure to handle the combination of penalty and solver correctly\n",
    "    # Objective function with LOO cross-validation\n",
    "    def objective(space):\n",
    "        # Conditional logic to ensure compatibility between solver and penalty\n",
    "        if space['penalty'] == 'elasticnet' and space['solver'] not in ['saga']:\n",
    "            return {'loss': 1, 'status': STATUS_OK}  # High loss for incompatible combination\n",
    "        if space['penalty'] == 'l1' and space['solver'] not in ['liblinear', 'saga']:\n",
    "            return {'loss': 1, 'status': STATUS_OK}\n",
    "        \n",
    "        # Instantiate the logistic regression model\n",
    "        model = LogisticRegression(\n",
    "            C=space['C'],\n",
    "            penalty=space['penalty'],\n",
    "            solver=space['solver'],\n",
    "            max_iter=1000  # Ensure convergence\n",
    "        )\n",
    "\n",
    "        # Using Leave-One-Out cross-validation\n",
    "        loo = LeaveOneOut()\n",
    "        score = cross_val_score(model, X, y, cv=loo, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "        # Return the loss\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "    # Run the optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=100,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Instantiate and fit the model with the best parameters\n",
    "    best_model = LogisticRegression(\n",
    "        C=best['C'],\n",
    "        penalty=['l1', 'l2', 'elasticnet', 'none'][best['penalty']],\n",
    "        solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'][best['solver']],\n",
    "        max_iter=1000\n",
    "    ).fit(X, y)\n",
    "\n",
    "    # Return the fitted model\n",
    "    return best_model\n",
    "perform_lr_parameter_tuning(X_en,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782febce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49%|                       | 49/100 [00:06<00:06,  7.50trial/s, best loss: -0.28888888888888886]"
     ]
    }
   ],
   "source": [
    "def perform_svm_parameter_tuning(X, y):\n",
    "    # Define the search space for SVM parameters\n",
    "    space = {\n",
    "        'C': hp.loguniform('C', -5, 15),  # Penalty parameter C of the error term\n",
    "        'kernel': hp.choice('kernel', ['rbf', 'sigmoid']),  # Specifies the kernel type to be used in the algorithm\n",
    "        'gamma': hp.choice('gamma', ['scale', 'auto']),  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "        'degree': hp.choice('degree', range(1, 6)),  # Degree of the polynomial kernel function ('poly')\n",
    "        'coef0': hp.uniform('coef0', 0, 10),  # Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'\n",
    "    }\n",
    "\n",
    "    # Objective function with LOO cross-validation\n",
    "    def objective(space):\n",
    "        classifier = SVC(\n",
    "            C=space['C'],\n",
    "            kernel=space['kernel'],\n",
    "            gamma=space['gamma'],\n",
    "            degree=space['degree'],\n",
    "            coef0=space['coef0'],\n",
    "            probability=True  # Enable probability estimates (necessary for certain evaluations)\n",
    "        )\n",
    "\n",
    "        # Using Leave-One-Out cross-validation\n",
    "        loo = LeaveOneOut()\n",
    "        accuracy = cross_val_score(classifier, X, y, cv=loo, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "        # Return the loss and status\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "    # Run the optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=100,  # Adjust based on computational budget\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Map `hp.choice` indices to their corresponding values\n",
    "    kernel_options = ['rbf', 'sigmoid']\n",
    "    gamma_options = ['scale', 'auto']\n",
    "\n",
    "    best_kernel = kernel_options[best['kernel']]\n",
    "    best_gamma = gamma_options[best['gamma']]\n",
    "    best_degree = best['degree'] + 1  # Adjust if needed\n",
    "\n",
    "    # Instantiate the SVM classifier with the best parameters\n",
    "    classifier = SVC(\n",
    "        C=best['C'],\n",
    "        kernel=best_kernel,\n",
    "        gamma=best_gamma,\n",
    "        degree=best_degree,\n",
    "        coef0=best['coef0'],\n",
    "        probability=True  # As mentioned above\n",
    "    )\n",
    "\n",
    "    # Train the SVM classifier with the best parameters\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    "perform_svm_parameter_tuning(X_en,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
