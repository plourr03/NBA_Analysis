{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb71a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from random import randint\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fe2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'localhost\\SQLEXPRESS'\n",
    "database = 'nba_game_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9449276",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id = '201142'\n",
    "pt_thresh = 25.5\n",
    "opp = 'BKN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362d2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "with base as (\n",
    "SELECT\n",
    "       pgl.[SEASON_YEAR]\n",
    "      ,pgl.[PLAYER_ID]\n",
    "      ,pgl.[PLAYER_NAME]\n",
    "      ,pgl.[NICKNAME]\n",
    "      ,pgl.[TEAM_ID]\n",
    "      ,pgl.[TEAM_ABBREVIATION]\n",
    "      ,pgl.[TEAM_NAME]\n",
    "      ,pgl.[GAME_ID]\n",
    "      ,pgl.[GAME_DATE]\n",
    "      ,LAG (pgl.[GAME_DATE]) OVER (PARTITION BY pgl.[PLAYER_ID] ORDER BY pgl.[GAME_DATE]) AS Last_Game_Played\n",
    "      ,pgl.[MATCHUP]\n",
    "      ,pgl.[WL]\n",
    "      ,pgl.[yearSeason]\n",
    "      ,CASE WHEN PGL.WL = 'W' THEN 1 ELSE 0 END AS WLInt\n",
    "      ,LAG (CASE WHEN PGL.WL = 'W' THEN 1 ELSE 0 END) OVER (PARTITION BY pgl.[PLAYER_ID] ORDER BY pgl.[GAME_DATE]) AS did_they_win_last_game\n",
    "     ,PTS\n",
    "  FROM [nba_game_data].[dbo].[PlayerGameLogs] pgl\n",
    "\n",
    "\n",
    "  WHERE pgl.PLAYER_ID = '{player_id}'\n",
    "  )\n",
    "  SELECT \n",
    " [PLAYER_ID]\n",
    ",[GAME_ID]\n",
    ",[GAME_DATE]\n",
    ",yearSeason\n",
    ",[PTS]\n",
    ",CASE WHEN [PTS] > {pt_thresh} THEN 1 ELSE 0 END AS PtsThreshold\n",
    ",SUM(WLInt) OVER (PARTITION BY PLAYER_ID ORDER BY GAME_DATE ROWS BETWEEN 11 PRECEDING AND 1 PRECEDING) AS RunningGamesWonLast10\n",
    ",DATEDIFF(day, Last_Game_Played,[GAME_DATE])-1 AS daysRest\n",
    ",did_they_win_last_game\n",
    "\n",
    "  FROM base\n",
    "\n",
    "  order by GAME_DATE\n",
    "\n",
    "'''\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "data = pd.read_sql(sql,cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f16d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "with base as (\n",
    "SELECT\n",
    "       pgl.[SEASON_YEAR]\n",
    "      ,pgl.[PLAYER_ID]\n",
    "      ,pgl.[PLAYER_NAME]\n",
    "      ,pgl.[NICKNAME]\n",
    "      ,pgl.[TEAM_ID]\n",
    "      ,pgl.[TEAM_ABBREVIATION]\n",
    "      ,pgl.[TEAM_NAME]\n",
    "      ,pgl.[GAME_ID]\n",
    "      ,pgl.[GAME_DATE]\n",
    "      ,LAG (pgl.[GAME_DATE]) OVER (PARTITION BY pgl.[PLAYER_ID] ORDER BY pgl.[GAME_DATE]) AS Last_Game_Played\n",
    "      ,pgl.[MATCHUP]\n",
    "      ,pgl.[WL]\n",
    "      ,CASE WHEN PGL.WL = 'W' THEN 1 ELSE 0 END AS WLInt\n",
    "  \n",
    "\n",
    "  FROM [nba_game_data].[dbo].[PlayerGameLogs] pgl\n",
    "\n",
    "\n",
    "\n",
    "  WHERE pgl.PLAYER_ID = '{player_id}'\n",
    "  )\n",
    "  SELECT \n",
    " [PLAYER_ID]\n",
    ",[GAME_ID]\n",
    ",SUM(WLInt) OVER (PARTITION BY PLAYER_ID ORDER BY GAME_DATE ROWS BETWEEN 11 PRECEDING AND CURRENT ROW) AS RunningGamesWonLast10\n",
    ",DATEDIFF(day, [GAME_DATE], GETDATE())-1 AS daysRest\n",
    ",WLInt AS did_they_win_last_game\n",
    "\n",
    "  FROM base\n",
    "\n",
    "  order by GAME_DATE\n",
    "\n",
    "'''\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "pred = pd.read_sql(sql,cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0667ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [5,20]\n",
    "\n",
    "for i in numbers:\n",
    "    temp = get_player_last_n_data(player_id,i)\n",
    "    temp = temp.drop(columns = ['TEAM_ABBREVIATION','oppAbrv'])\n",
    "    data = pd.merge(data, temp, on=['PLAYER_ID', 'GAME_ID'])\n",
    "    del temp\n",
    "    \n",
    "    temp = get_pred_data(player_id,i,opp)\n",
    "    \n",
    "    pred = pd.merge(pred, temp, on=['PLAYER_ID', 'GAME_ID'])\n",
    "    del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5861cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.iloc[-1:].drop(columns=['PLAYER_ID','GAME_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02fa88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77619a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data.loc[(data['PTS'] < (pt_thresh - 3)) | (data['PTS'] > (pt_thresh + 3))]#['PTS']\n",
    "df_season_buffer = filtered.sort_values(by='GAME_DATE').tail(82)#.loc[filtered['yearSeason']=='2024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364ae241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAIOCAYAAACWIeTWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzuUlEQVR4nO3df1RVdb7/8dfhcDxyENT8AZKUKDhW0OSPQi0FcmAGGkfiWk5Wo7epqazumPkjdc2EjRdGM3TWmN3sttSmqzk55DSoXVhliGOU2nRLy8Yaf36VXJoJAuLhsL9/eDk3BjJADpsPPh9rsep89j6bN7TWnuds9jnHYVmWJQAAAMAAQXYPAAAAADQX8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKwGirV6+Ww+Fo8NWnTx8lJyeroKCg3ed55513GszidDoVERGhO+64Q59++ql/v4MHD8rhcGj16tUt/h6ffPKJsrOzdfDgwbYb/H+99dZbGjFihEJDQ+VwOLRx48Ym96ufv/4rKChIvXr1UkZGht59911J0tSpUxv9t2nqa+rUqZIkr9erF154QTfeeKOuuOIKeTweXX311ZowYYJef/31Nv9ZAZgp2O4BAKAtrFq1SkOGDJFlWSorK9Py5cs1fvx4vfHGGxo/fny7z5OTk6OUlBSdP39eu3bt0tNPP6233npLH3/8sa688spLOvYnn3yiBQsWKDk5WQMGDGibgSVZlqU777xTgwcP1htvvKHQ0FB973vfu+hzHnvsMU2ePFk+n0979+7VggULlJKSonfffVe/+tWv9NBDD/n3/eCDD/TII4/4fzf1+vTpI0m69957lZ+fr+nTp2vBggVyu936xz/+oTfffFP//d//rdtvv73NflYA5iJeAXQK8fHxGjFihP/xj370I/Xs2VPr1q2zJV7j4uI0cuRISdLYsWPVo0cP/fznP9fq1as1f/78dp+nOY4dO6avvvpKt99+u8aNG9es51x11VX+n/Pmm29WbGysxo0bpxUrVujFF1/UoEGD/PueO3dOUsPfTb0DBw5o/fr1+vWvf60FCxb418eNG6cHHnhAdXV1l/rjAegkuG0AQKfUtWtXdenSRS6Xq8H6V199pWnTpunKK69Uly5dNHDgQM2fP181NTWSLgTW0KFDFRsbqzNnzvifV1ZWpsjISCUnJ8vn87V4nvpYO3To0EX32759u8aNG6ewsDB5PB6NHj1amzZt8m9fvXq17rjjDklSSkqK/0/v33X7wXcdNzs7W/3795ckzZkzRw6Ho1VXdZv7c/6zU6dOSZL69evX5PagIP7nCsAFnA0AdAo+n0+1tbXyer06evSopk+frsrKSk2ePNm/z7lz55SSkqKXX35ZM2bM0KZNm3TPPfdo8eLFysrKknQhev/4xz/qxIkTuu+++yRJdXV1uvvuu2VZltatWyen09ni+T7//HNJ//cn8qYUFxfr1ltv1ZkzZ/TSSy9p3bp1CgsL0/jx47V+/XpJ0m233aacnBxJ0nPPPad3331X7777rm677bZLOu7999+v/Px8SRduBXj33XdbdZ9pc37OplxzzTXq0aOHFixYoJUrVwbkfl4AnYQFAAZbtWqVJanRl9vttlasWNFg3//4j/+wJFl//OMfG6wvWrTIkmQVFhb619avX29JspYtW2b9+te/toKCghps/zZbt261JFnr16+3vF6vVVVVZW3bts2KjY21nE6n9T//8z+WZVnWgQMHLEnWqlWr/M8dOXKk1bdvX6uiosK/Vltba8XHx1v9+/e36urqLMuyrNdee82SZG3durVZv6PmHrd+pmeeeeY7j1m/76JFiyyv12udO3fO2r17t3XjjTdakqxNmzZ96+/mtddea/KYmzZtsnr37u3/b9irVy/rjjvusN54441m/ZwALg9ceQXQKbz88svauXOndu7cqS1btmjKlCl65JFHtHz5cv8+b7/9tkJDQzVx4sQGz61/tftbb73lX7vzzjv18MMPa9asWVq4cKHmzZun1NTUZs8zadIkuVwueTwejR07Vj6fTxs2bND111/f5P6VlZV67733NHHiRHXr1s2/7nQ6de+99+ro0aP67LPPmv39A33cenPmzJHL5VLXrl01fPhwHT58WC+88IIyMjJafKyMjAwdPnxYr7/+umbOnKnrrrtOGzdu1E9+8hM9+uijrZ4RQOfCC7YAdArXXHNNoxdsHTp0SLNnz9Y999yjHj166NSpU4qMjJTD4Wjw3L59+yo4ONh/32W9++67T88//7y6dOmif/u3f2vRPIsWLdKtt94qp9Op3r17Kzo6+qL7nz59WpZlNXnPZ1RUlCQ1mq85AnXcer/85S91zz33KCgoSD169FBMTEyj329LhISEKDMzU5mZmZKkw4cPKz09Xc8995wefvhhXXfdda0+NoDOgSuvADqt66+/XtXV1fr73/8uSerVq5e+/PJLWZbVYL8TJ06otrZWvXv39q9VVlbq3nvv1eDBgxUSEqL777+/Rd974MCBGjFihIYOHfqd4SpJPXv2VFBQkI4fP95o27FjxySpwXzNFajj1uvfv79GjBihYcOGaeDAgZcUrk256qqr9Itf/EKStHfv3jY9NgAzEa8AOq0PP/xQ0v+9eGjcuHE6e/Zsozfef/nll/3b6z300EM6fPiw8vPz9dJLL+mNN97Q0qVLAzZraGioEhMTlZ+fr+rqav96XV2dXnnlFfXv31+DBw+WJLndbklqsF9bHNdOFRUVOnv2bJPb6j/cof5KMYDLG7cNAOgU9uzZo9raWkkX/gyen5+voqIi3X777YqJiZEk/exnP9Nzzz2nKVOm6ODBg0pISND27duVk5OjjIwM/eAHP5Ak/ed//qdeeeUVrVq1Stddd52uu+46Pfroo5ozZ45uvvlm3XTTTQH5GXJzc5WamqqUlBTNnDlTXbp00YoVK7Rnzx6tW7fOf1UzPj5ekrRy5UqFhYWpa9euiomJUa9evS7puHb67LPP9MMf/lA//elPlZSUpH79+un06dPatGmTVq5cqeTkZI0ePdruMQF0BDa/YAwALklT7zbQvXt364YbbrDy8vKsc+fONdj/1KlT1kMPPWT169fPCg4Otq6++mpr7ty5/v0++ugjKyQkxJoyZUqD5507d84aPny4NWDAAOv06dPfOs93vaK+XlPvNmBZllVSUmLdeuutVmhoqBUSEmKNHDnS+stf/tLo+cuWLbNiYmIsp9PZ5HH+WXOO25p3G2jOvvUu9rs5ffq0tXDhQuvWW2+1rrzySqtLly5WaGiodcMNN1gLFy60qqqqmv19AHRuDsv6p5u/AAAAgA6Ke14BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADG6PQfUlBXV6djx44pLCysQ7wRNwAAABqyLEsVFRWKiopSUNDFr612+ng9duxYsz5XHAAAAPY6cuSI+vfvf9F9On28hoWFSbrwywgPD7d5GgC4dF6vV4WFhUpLS5PL5bJ7HAC4ZOXl5YqOjvZ328V0+nitv1UgPDyceAXQKXi9Xnk8HoWHhxOvADqV5tziyQu2AAAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAM4vP5VFxcrG3btqm4uFg+n8/ukQCgXRGvAGCI/Px8xcbGKjU1VXl5eUpNTVVsbKzy8/PtHg0A2g3xCgAGyM/P18SJE5WQkKCSkhKtW7dOJSUlSkhI0MSJEwlYAJcNh2VZlt1DBFJ5ebm6d++uM2fOKDw83O5xAKDFfD6fYmNjlZCQoI0bN8rn82nz5s3KyMiQ0+lUZmam9uzZo/3798vpdNo9LgC0WEt6jSuvANDBlZSU6ODBg5o3b56CghqetoOCgjR37lwdOHBAJSUlNk0IAO2HeAWADu748eOSpPj4+Ca316/X7wcAnRnxCgAdXL9+/SRJe/bsaXJ7/Xr9fgDQmRGvANDBjRkzRgMGDFBOTo7q6uoabKurq1Nubq5iYmI0ZswYmyYEgPZDvAJAB+d0OvXss8+qoKBAmZmZKi0tVXV1tUpLS5WZmamCggItWbKEF2sBuCwE2z0AAOC7ZWVlacOGDXriiSc0duxY/3pMTIw2bNigrKwsG6cDgPbDW2UBgEF8Pp+2bt2qLVu2KD09XSkpKVxxBWC8lvQaV14BwCBOp1NJSUmqrKxUUlIS4QrgssM9rwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAGAQn8+n4uJibdu2TcXFxfL5fHaPBADtingFAEPk5+crNjZWqampysvLU2pqqmJjY5Wfn2/3aADQbohXADBAfn6+Jk6cqISEBJWUlGjdunUqKSlRQkKCJk6cSMACuGzwIQUA0MH5fD7FxsYqISFBGzdulM/n0+bNm5WRkSGn06nMzEzt2bNH+/fv531fARipJb3GlVcA6OBKSkp08OBBzZs3T0FBDU/bQUFBmjt3rg4cOKCSkhKbJgSA9kO8AkAHd/z4cUlSfHx8k9vr1+v3A4DOjHgFgA6uX79+kqQ9e/Y0ub1+vX4/AOjMiFcA6ODGjBmjAQMGKCcnR3V1dQ221dXVKTc3VzExMRozZoxNEwJA+yFeAaCDczqdevbZZ1VQUKDMzEyVlpaqurpapaWlyszMVEFBgZYsWcKLtQBcFoLtHgAA8N2ysrK0YcMGPfHEExo7dqx/PSYmRhs2bFBWVpaN0wFA++GtsgDAID6fT1u3btWWLVuUnp6ulJQUrrgCMF5Leo0rrwBgEKfTqaSkJFVWViopKYlwBXDZ4Z5XAAAAGIN4BQAAgDGIVwAAABiDe14BwCDnz5/X73//e7399tv6/PPP9dhjj6lLly52jwUA7YYrrwBgiNmzZys0NFQzZ87U5s2bNXPmTIWGhmr27Nl2jwYA7YYrrwBggNmzZ+uZZ55RRESEFixYILfbrZqaGj311FN65plnJEmLFy+2eUoACDze5xUAOrjz588rNDRUvXr10tGjR2VZljZv3qyMjAw5HA71799fp06dUmVlJbcQADBSS3qN2wYAoINbsWKFamtrtXDhQgUHN/yDWXBwsJ5++mnV1tZqxYoVNk0IAO2HeAWADu6LL76QJP34xz9ucnv9ev1+ANCZEa8A0MENGjRIklRQUNDk9vr1+v0AoDPjnlcA6OC+ec/roUOHVFJSoi1btig9PV1jxozR1VdfzT2vAIxmzD2v2dnZcjgcDb4iIyP92y3LUnZ2tqKiohQSEqLk5GTt3bvXxokBoP116dJFjz/+uL788kt5PB6lpqYqLy9Pqamp8ng8+vLLL/X4448TrgAuC7bfNnDdddfp+PHj/q+PP/7Yv23x4sXKy8vT8uXLtXPnTkVGRio1NVUVFRU2TgwA7W/kyJGSLvyf+m+qf1y/HQA6O9vjNTg4WJGRkf6vPn36SLpwQl62bJnmz5+vrKwsxcfHa82aNaqqqtLatWttnhoA2o/P59MTTzyh8ePHq6qqSkuWLFFGRoaWLFmiqqoqjR8/XjNnzpTP57N7VAAIONs/pGD//v2KioqS2+1WYmKicnJyNHDgQB04cEBlZWVKS0vz7+t2u5WUlKQdO3bowQcfbPJ4NTU1qqmp8T8uLy+XJHm9Xnm93sD+MAAQAMXFxTp48KD+8Ic/yOl06uGHH1ZsbKxSU1PldDo1a9YsjR07Vlu3blVSUpLd4wJAi7Wk0WyN18TERL388ssaPHiwvvzySy1cuFCjR4/W3r17VVZWJkmKiIho8JyIiAgdOnToW4+Zm5urBQsWNFovLCyUx+Np2x8AANrBtm3bJElHjx7VqVOn/OtFRUWSpOrqaknSli1bVFlZ2f4DAsAlqqqqava+tsZrenq6/98TEhI0atQoDRo0SGvWrPHfv+VwOBo8x7KsRmvfNHfuXM2YMcP/uLy8XNHR0UpLS+PdBgAYKTQ0VHl5eerfv78SExPl9XpVVFSk1NRUuVwulZaWSrpwTuXKKwAT1f+lvDlsv23gm0JDQ5WQkKD9+/crMzNTklRWVqZ+/fr59zlx4kSjq7Hf5Ha75Xa7G627XC65XK42nxkAAi0lJUUDBgzQ4sWLtXHjRv+6y+WS0+nUM888o5iYGKWkpMjpdNo3KAC0UksarUPFa01NjT799FONGTNGMTExioyMVFFRkYYOHSrpwnsdFhcXa9GiRTZPCgDtx+l06tlnn9XEiRM1YcIEpaamav/+/Tp06JCKioq0adMmbdiwgXAFcFmwNV5nzpyp8ePH66qrrtKJEye0cOFClZeXa8qUKXI4HJo+fbpycnIUFxenuLg45eTkyOPxaPLkyXaODQDtLisrSzNnztTSpUsbfNJWcHCwZs6cqaysLBunA4D2Y2u8Hj16VHfddZdOnjypPn36aOTIkSotLdXVV18tSZo9e7aqq6s1bdo0nT59WomJiSosLFRYWJidYwNAu8vPz9eSJUt02223+a+8xsXFqaioSEuWLNHIkSMJWACXBT4eFgA6OJ/Pp9jYWCUkJGjjxo3y+XzavHmzMjIy5HQ6lZmZqT179mj//v3cOgDASMZ8PCwA4LuVlJTo4MGDmjdvnoKCGp62g4KCNHfuXB04cEAlJSU2TQgA7Yd4BYAO7vjx45Kk+Pj4JrfXr9fvBwCdGfEKAB1c/dsF7tmzp8nt9evffFtBAOisiFcA6ODGjBmjAQMGKCcnR3V1dQ221dXVKTc3VzExMRozZoxNEwJA+yFeAaCDq3+f14KCAmVmZqq0tFTV1dUqLS1VZmamCgoKtGTJEl6sBeCy0KE+pAAA0LSsrCxt2LBBTzzxhMaOHetfj4mJ0YYNG3ibLACXDd4qCwAM4vP5tHXrVm3ZskXp6el8JCyATqElvcaVVwAwiNPpVFJSkiorK5WUlES4ArjscM8rAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMEWz3AADQGVVVVWnfvn0BOXZFRYWKi4vVo0cPhYWFBeR7DBkyRB6PJyDHBoBLQbwCQADs27dPw4cPD+j3WLp0acCOvXv3bg0bNixgxweA1iJeASAAhgwZot27dwfk2Hv27NGUKVO0Zs0axcfHB+R7DBkyJCDHBYBLRbwCQAB4PJ6AXbmsra2VdCEwuToK4HLDC7YAAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADG6DDxmpubK4fDoenTp/vXLMtSdna2oqKiFBISouTkZO3du9e+IQEAAGCrDhGvO3fu1MqVK3X99dc3WF+8eLHy8vK0fPly7dy5U5GRkUpNTVVFRYVNkwIAAMBOtsfr2bNndffdd+vFF19Uz549/euWZWnZsmWaP3++srKyFB8frzVr1qiqqkpr1661cWIAAADYxfZ4feSRR3TbbbfpBz/4QYP1AwcOqKysTGlpaf41t9utpKQk7dixo73HBAAAQAcQbOc3f/XVV7V7927t2rWr0baysjJJUkRERIP1iIgIHTp06FuPWVNTo5qaGv/j8vJySZLX65XX622LsQHAVvXnMs5rADqLlpzLbIvXI0eO6Je//KUKCwvVtWvXb93P4XA0eGxZVqO1b8rNzdWCBQsarRcWFsrj8bR+YADoIL744gtJ0nvvvaeTJ0/aPA0AXLqqqqpm7+uwLMsK4CzfauPGjbr99tvldDr9az6fTw6HQ0FBQfrss88UGxurDz74QEOHDvXvM2HCBPXo0UNr1qxp8rhNXXmNjo7WyZMnFR4eHrgfCADayfvvv69bbrlF27dv10033WT3OABwycrLy9W7d2+dOXPmO3vNtiuv48aN08cff9xg7V//9V81ZMgQzZkzRwMHDlRkZKSKior88Xr+/HkVFxdr0aJF33pct9stt9vdaN3lcsnlcrXtDwEANqg/l3FeA9BZtORcZlu8hoWFKT4+vsFaaGioevXq5V+fPn26cnJyFBcXp7i4OOXk5Mjj8Wjy5Ml2jAwAAACb2fqCre8ye/ZsVVdXa9q0aTp9+rQSExNVWFiosLAwu0cDAACADTpUvL7zzjsNHjscDmVnZys7O9uWeQAAANCx2P4+rwAAAEBzEa8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADBGsN0DAICd9u/fr4qKCrvHaJF9+/b5/xkcbN5pPCwsTHFxcXaPAcBQ5p31AKCN7N+/X4MHD7Z7jFabMmWK3SO02t///ncCFkCrEK8ALlv1V1xfeeUVXXPNNTZP03xnz57Vxo0blZmZqW7dutk9Tot8+umnuueee4y72g2g4yBeAVz2rrnmGg0bNszuMZrN6/Xq9OnTGjVqlFwul93jAEC74gVbAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGPYGq/PP/+8rr/+eoWHhys8PFyjRo3Sli1b/Nsty1J2draioqIUEhKi5ORk7d2718aJAQAAYCdb47V///767W9/q127dmnXrl269dZbNWHCBH+gLl68WHl5eVq+fLl27typyMhIpaam8rGCAAAAlylb43X8+PHKyMjQ4MGDNXjwYP37v/+7unXrptLSUlmWpWXLlmn+/PnKyspSfHy81qxZo6qqKq1du9bOsQEAAGCTYLsHqOfz+fTaa6+psrJSo0aN0oEDB1RWVqa0tDT/Pm63W0lJSdqxY4cefPDBJo9TU1Ojmpoa/+Py8nJJFz4L3Ov1BvaHAGCU2tpa/z9NOj/Uz2rSzPVM/Z0DCKyWnA9sj9ePP/5Yo0aN0rlz59StWze9/vrruvbaa7Vjxw5JUkRERIP9IyIidOjQoW89Xm5urhYsWNBovbCwUB6Pp22HB2C0L774QpK0fft2HT9+3OZpWq6oqMjuEVrM9N85gMCoqqpq9r5tFq9ff/21evTo0eLnfe9739OHH36or7/+Wn/60580ZcoUFRcX+7c7HI4G+1uW1Wjtm+bOnasZM2b4H5eXlys6OlppaWkKDw9v8XwAOq+//e1vkqRbbrlFQ4cOtXma5vN6vSoqKlJqaqpcLpfd47SIqb9zAIFV/5fy5mhVvC5atEgDBgzQpEmTJEl33nmn/vSnPykyMlKbN2/W97///WYfq0uXLoqNjZUkjRgxQjt37tTvfvc7zZkzR5JUVlamfv36+fc/ceJEo6ux3+R2u+V2uxutu1wu407yAAIrODjY/08Tzw8mntdM/50DCIyWnA9a9YKtF154QdHR0ZIu/NmqqKhIW7ZsUXp6umbNmtWaQ/pZlqWamhrFxMQoMjKywZ/Fzp8/r+LiYo0ePfqSvgcAAADM1Korr8ePH/fHa0FBge68806lpaVpwIABSkxMbPZx5s2bp/T0dEVHR6uiokKvvvqq3nnnHb355ptyOByaPn26cnJyFBcXp7i4OOXk5Mjj8Wjy5MmtGRsAAACGa1W89uzZU0eOHFF0dLTefPNNLVy4UNKFq6Y+n6/Zx/nyyy9177336vjx4+revbuuv/56vfnmm0pNTZUkzZ49W9XV1Zo2bZpOnz6txMREFRYWKiwsrDVjAwAAwHCtitesrCxNnjxZcXFxOnXqlNLT0yVJH374of/+1eZ46aWXLrrd4XAoOztb2dnZrRkTAAAAnUyr4nXp0qUaMGCAjhw5osWLF6tbt26SLtxOMG3atDYdEAAAAKjXqnh99913NX36dP+rRus9+uij/vdnBQAAANpaq95tICUlRV999VWj9TNnziglJeWShwIAAACa0qp4/bYPCjh16pRCQ0MveSgAAACgKS26bSArK0vShRdSTZ06tcGHAfh8Pn300Ue8BysAAAACpkXx2r17d0kXrryGhYUpJCTEv61Lly4aOXKkHnjggbadEAAAAPhfLYrXVatWSZIGDBigmTNncosAAAAA2lWr3m3gqaeeaus5AAAAgO/Uqhds1X8yVlRUlIKDg+V0Oht8AQAAAIHQqiuvU6dO1eHDh/WrX/1K/fr1a/KdBwAAAIC21qp43b59u0pKSnTDDTe08TgAAADAt2vVbQPR0dGyLKutZwEAAAAuqlXxumzZMj355JM6ePBgG48DAAAAfLtW3TYwadIkVVVVadCgQfJ4PHK5XA22N/XRsQAAAMClalW8Llu2rI3HAAAAAL5bq+J1ypQpbT0HALQ7R+05DY0MUsjXf5eOteouKnvU1qp71UHp+P9Iwa06jdsm5Ou/a2hkkBy15+weBYChmn3WKy8vV3h4uP/fL6Z+PwDoyLqePawPHuwmbXtQ2mb3NM3nkpQsSZ/ZO0drXCPpgwe76dOzhyWNtnscAAZqdrz27NlTx48fV9++fdWjR48m39vVsiw5HA75fL42HRIAAuFct6s07IWz+q//+i9dM2SI3eM0m7e2Vn/961918803y2XYlddP9+3T3XffrZcyrrJ7FACGavZZ7+2339YVV1whSdq6dWvABgKA9mIFd9XfyupU3WOwFHWD3eM0n9erM57/J/X7vvRPL5jt6KrL6vS3sjpZwV3tHgWAoZodr0lJSU3+OwAAANBeWv33pq+//lovvfSSPv30UzkcDl177bW677771L1797acDwAAAPBr1ctrd+3apUGDBmnp0qX66quvdPLkSeXl5WnQoEH64IMP2npGAAAAQFIrr7w+/vjj+slPfqIXX3xRwf/7YoHa2lrdf//9mj59urZtM+hluwAAADBGq+J1165dDcJVkoKDgzV79myNGDGizYYDAAAAvqlVtw2Eh4fr8OHDjdaPHDmisLCwSx4KAAAAaEqr4nXSpEn6+c9/rvXr1+vIkSM6evSoXn31Vd1///2666672npGAAAAQFIrbxtYsmSJgoKC9LOf/Uy1tbWSJJfLpYcffli//e1v23RAAAAAoF6L4rWqqkqzZs3Sxo0b5fV6lZmZqUcffVTdu3dXbGysPB5PoOYEAAAAWhavTz31lFavXq27775bISEhWrt2rerq6vTaa68Faj4AAADAr0Xxmp+fr5deekk//elPJUl33323br75Zvl8PjmdzoAMCAAAANRr0Qu2jhw5ojFjxvgf33TTTQoODtaxY8fafDAAAADgn7UoXn0+n7p06dJgLTg42P+iLQAAACCQWnTbgGVZmjp1qtxut3/t3LlzeuihhxQaGupfy8/Pb7sJAQAAgP/VonidMmVKo7V77rmnzYYBAAAALqZF8bpq1apAzQEAAAB8p1Z9whYAAABgB+IVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BAABgDOIVAAAAxiBeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGCLZ7AACwS1VVlSTpgw8+sHmSljl79qyKi4vVs2dPdevWze5xWuTTTz+1ewQAhiNeAVy29u3bJ0l64IEHbJ6kdZYuXWr3CK0WFhZm9wgADEW8ArhsZWZmSpKGDBkij8dj7zAtsGfPHk2ZMkVr1qxRfHy83eO0WFhYmOLi4uweA4ChiFcAl63evXvr/vvvt3uMFqutrZV0IbqHDRtm8zQA0L54wRYAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGLbGa25urm688UaFhYWpb9++yszM1GeffdZgH8uylJ2draioKIWEhCg5OVl79+61aWIAAADYydZ4LS4u1iOPPKLS0lIVFRWptrZWaWlpqqys9O+zePFi5eXlafny5dq5c6ciIyOVmpqqiooKGycHAACAHWz9eNg333yzweNVq1apb9++2r17t8aOHSvLsrRs2TLNnz9fWVlZkqQ1a9YoIiJCa9eu1YMPPmjH2AAAALBJh7rn9cyZM5KkK664QpJ04MABlZWVKS0tzb+P2+1WUlKSduzYYcuMAAAAsI+tV16/ybIszZgxQ7fccovi4+MlSWVlZZKkiIiIBvtGRETo0KFDTR6npqZGNTU1/sfl5eWSJK/XK6/XG4jRAaBd1Z/LOK8B6Cxaci7rMPH66KOP6qOPPtL27dsbbXM4HA0eW5bVaK1ebm6uFixY0Gi9sLBQHo+nbYYFABt98cUXkqT33ntPJ0+etHkaALh0VVVVzd63Q8TrY489pjfeeEPbtm1T//79/euRkZGSLlyB7devn3/9xIkTja7G1ps7d65mzJjhf1xeXq7o6GilpaUpPDw8QD8BALSf999/X5KUmJiom266yeZpAODS1f+lvDlsjVfLsvTYY4/p9ddf1zvvvKOYmJgG22NiYhQZGamioiINHTpUknT+/HkVFxdr0aJFTR7T7XbL7XY3Wne5XHK5XG3/QwBAO6s/l3FeA9BZtORcZmu8PvLII1q7dq3+/Oc/KywszH+Pa/fu3RUSEiKHw6Hp06crJydHcXFxiouLU05OjjwejyZPnmzn6AAAALCBrfH6/PPPS5KSk5MbrK9atUpTp06VJM2ePVvV1dWaNm2aTp8+rcTERBUWFiosLKydpwUAAIDdbL9t4Ls4HA5lZ2crOzs78AMBAACgQ+tQ7/MKAAAAXAzxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwhq3xum3bNo0fP15RUVFyOBzauHFjg+2WZSk7O1tRUVEKCQlRcnKy9u7da8+wAAAAsJ2t8VpZWanvf//7Wr58eZPbFy9erLy8PC1fvlw7d+5UZGSkUlNTVVFR0c6TAgAAoCMItvObp6enKz09vcltlmVp2bJlmj9/vrKysiRJa9asUUREhNauXasHH3ywPUcFAABAB9Bh73k9cOCAysrKlJaW5l9zu91KSkrSjh07bJwMAAAAdrH1yuvFlJWVSZIiIiIarEdEROjQoUPf+ryamhrV1NT4H5eXl0uSvF6vvF5vACYFgPZVfy7jvAags2jJuazDxms9h8PR4LFlWY3Wvik3N1cLFixotF5YWCiPx9Pm8wFAe/viiy8kSe+9955Onjxp8zQAcOmqqqqavW+HjdfIyEhJF67A9uvXz79+4sSJRldjv2nu3LmaMWOG/3F5ebmio6OVlpam8PDwwA0MAO3k/ffflyQlJibqpptusnkaALh09X8pb44OG68xMTGKjIxUUVGRhg4dKkk6f/68iouLtWjRom99ntvtltvtbrTucrnkcrkCNi8AtJf6cxnnNQCdRUvOZbbG69mzZ/X555/7Hx84cEAffvihrrjiCl111VWaPn26cnJyFBcXp7i4OOXk5Mjj8Wjy5Mk2Tg0AAAC72Bqvu3btUkpKiv9x/Z/7p0yZotWrV2v27Nmqrq7WtGnTdPr0aSUmJqqwsFBhYWF2jQwAAAAb2RqvycnJsizrW7c7HA5lZ2crOzu7/YYCAABAh9Vh3+cVAAAA+GfEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxh64cUAEBnVVVVpX379gXk2PXH3bdvn4KDA3MaHzJkiDweT0CODQCXgngFgADYt2+fhg8fHtDvMWXKlIAde/fu3Ro2bFjAjg8ArUW8AkAADBkyRLt37w7IsSsqKvTnP/9ZEyZMUFhYWEC+x5AhQwJyXAC4VMQrAASAx+MJ2JVLr9err7/+WqNHj5bL5QrI9wCAjooXbAEAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAYxOfzqbi4WNu2bVNxcbF8Pp/dIwFAuzIiXlesWKGYmBh17dpVw4cPV0lJid0jAUC7y8/PV2xsrFJTU5WXl6fU1FTFxsYqPz/f7tEAoN10+Hhdv369pk+frvnz5+tvf/ubxowZo/T0dB0+fNju0QCg3eTn52vixIlKSEhQSUmJ1q1bp5KSEiUkJGjixIkELIDLhsOyLMvuIS4mMTFRw4YN0/PPP+9fu+aaa5SZmanc3NzvfH55ebm6d++uM2fOKDw8PJCjAkBA+Hw+xcbGKiEhQRs3bpTP59PmzZuVkZEhp9OpzMxM7dmzR/v375fT6bR7XABosZb0WnA7zdQq58+f1+7du/Xkk082WE9LS9OOHTuafE5NTY1qamr8j8vLyyVJXq9XXq83cMMCQIAUFxfr4MGD+sMf/iCfz+c/l9X/c9asWRo7dqy2bt2qpKQkO0cFgFZpSaN16Hg9efKkfD6fIiIiGqxHRESorKysyefk5uZqwYIFjdYLCwvl8XgCMicABNK2bdskSUePHtWpU6f860VFRZKk6upqSdKWLVtUWVnZ/gMCwCWqqqpq9r4dOl7rORyOBo8ty2q0Vm/u3LmaMWOG/3F5ebmio6OVlpbGbQMAjBQaGqq8vDz1799fiYmJ8nq9KioqUmpqqlwul0pLSyVJ6enpXHkFYKT6v5Q3R4eO1969e8vpdDa6ynrixIlGV2Prud1uud3uRusul0sulysgcwJAIKWkpGjAgAFavHixNm7c6F93uVxyOp165plnFBMTo5SUFO55BWCkljRah363gS5dumj48OH+P43VKyoq0ujRo22aCgDal9Pp1LPPPquCggJlZmaqtLRU1dXVKi0tVWZmpgoKCrRkyRLCFcBloUNfeZWkGTNm6N5779WIESM0atQorVy5UocPH9ZDDz1k92gA0G6ysrK0YcMGPfHEExo7dqx/PSYmRhs2bFBWVpaN0wFA++nw8Tpp0iSdOnVKTz/9tI4fP674+Hht3rxZV199td2jAUC7ysrK0oQJE7R161Zt2bJF6enp3CoA4LLT4d/n9VLxPq8AOhuv1+t/n1fu5QfQGbSk1zr0Pa8AAADANxGvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYxCvAAAAMAbxCgAAAGMQrwAAADAG8QoAAABjEK8AAAAwBvEKAAAAYwTbPUCgWZYlSSovL7d5EgBoG16vV1VVVSovL5fL5bJ7HAC4ZPWdVt9tF9Pp47WiokKSFB0dbfMkAAAAuJiKigp17979ovs4rOYkrsHq6up07NgxhYWFyeFw2D0OAFyy8vJyRUdH68iRIwoPD7d7HAC4ZJZlqaKiQlFRUQoKuvhdrZ0+XgGgsykvL1f37t115swZ4hXAZYcXbAEAAMAYxCsAAACMQbwCgGHcbreeeuopud1uu0cBgHbHPa8AAAAwBldeAQAAYAziFQAAAMYgXgEAAGAM4hUAAADGIF4BoAOYOnWqHA6HHA6HXC6XBg4cqJkzZ2rWrFn+9W/7OnjwoCorKzVnzhwNHDhQXbt2VZ8+fZScnKyCggK7fzQAaFPBdg8AALjgRz/6kVatWiWv16uSkhLdf//9mjRpko4fP+7f58Ybb9QvfvELPfDAA/61Pn36aOrUqXr//fe1fPlyXXvttTp16pR27NihU6dO2fGjAEDAEK8A0EG43W5FRkZKkiZPnqytW7eqoKBAq1at8u/jdDoVFhbm36/eX/7yF/3ud79TRkaGJGnAgAEaPnx4+w0PAO2E2wYAoIMKCQmR1+tt1r6RkZHavHmzKioqAjwVANiLeAWADuj999/X2rVrNW7cuGbtv3LlSu3YsUO9evXSjTfeqMcff1x//etfAzwlALQ/4hUAOoiCggJ169ZNXbt21ahRozR27Fj9/ve/b9Zzx44dq3/84x9666239C//8i/au3evxowZo9/85jcBnhoA2hfxCgAdREpKij788EN99tlnOnfunPLz89W3b99mP9/lcmnMmDF68sknVVhYqKefflq/+c1vdP78+QBODQDtixdsAUAHERoaqtjY2DY73rXXXqva2lqdO3dOXbp0abPjAoCdiFcA6ASSk5N11113acSIEerVq5c++eQTzZs3TykpKQoPD7d7PABoM8QrAHQCP/zhD7VmzRrNmzdPVVVVioqK0o9//GP9+te/tns0AGhTDsuyLLuHAAAAAJqDF2wBAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACMQbwCAADAGMQrAAAAjEG8AgAAwBjEKwAAAIxBvAIAAMAYxCsAAACM8f8BPpPXftgU5VoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a box plot for the 'PTS' column\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data['PTS'])\n",
    "plt.title('Box Plot of PTS')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks([1], ['PTS'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7005ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine outliers using the IQR method\n",
    "Q1 = data['PTS'].quantile(0.25)\n",
    "Q3 = data['PTS'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for the outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "data = data[(data['PTS'] >= lower_bound) &\n",
    "                                           (data['PTS'] <= upper_bound)]\n",
    "\n",
    "# Creating a new boxplot without the outliers\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data['PTS'])\n",
    "plt.title('Box Plot of PTS without Outliers')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks([1], ['PTS'])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7b8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_60 = data.tail(60)\n",
    "df_last_60 = df_last_60.drop(columns=['PLAYER_ID','GAME_ID','GAME_DATE','PTS','yearSeason'])\n",
    "df_season = data.loc[data['yearSeason']=='2024']\n",
    "df_season = df_season.drop(columns=['PLAYER_ID','GAME_ID','GAME_DATE','PTS','yearSeason'])\n",
    "df = data.drop(columns=['PLAYER_ID','GAME_ID','GAME_DATE','PTS','yearSeason'])\n",
    "df = df.iloc[1:]\n",
    "df_season_buffer = df_season_buffer.drop(columns=['PLAYER_ID','GAME_ID','GAME_DATE','PTS','yearSeason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "import numpy as np\n",
    "\n",
    "# Function to select top 'n' features\n",
    "def select_top_n_features(X, y, n):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    selector = SelectFromModel(model, max_features=n, prefit=True)\n",
    "    X_selected = selector.transform(X)\n",
    "    return X_selected, selector\n",
    "\n",
    "# Modified function to evaluate models with feature selection\n",
    "def evaluate_models_with_feature_selection(df_feat, X, y, n_features, dataset_type, type_data):\n",
    "    global final_model, final_model_cv_score, final_model_score, final_scoring_type, final_dataset, type_of_data, base_score, final_X, final_y, final_x_transformer\n",
    "\n",
    "    X_selected, selector = select_top_n_features(X, y, n_features)\n",
    "    scoring_method = 'precision'  # Change to 'f1_macro', 'f1_micro', or 'f1_weighted' for multi-class\n",
    "\n",
    "    if len(df_feat) < 100:\n",
    "        cv_strategy = LeaveOneOut()\n",
    "    else:\n",
    "        cv_strategy = LeaveOneOut()#KFold(n_splits=15, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        cv_scores = np.mean(cross_val_score(model, X_selected, y, cv=cv_strategy, scoring=scoring_method, n_jobs=-1))\n",
    "        print(name,cv_scores)\n",
    "        if name == 'Bayesian' and cv_scores == final_model_cv_score:\n",
    "            cv_scores += 0.001\n",
    "\n",
    "        if cv_scores > final_model_cv_score:\n",
    "            final_model = name\n",
    "            final_model_cv_score = cv_scores\n",
    "            final_model_score = cv_scores - base_score\n",
    "            final_scoring_type = 'loo' if len(df_feat) < 100 else 'cv'\n",
    "            final_dataset = dataset_type\n",
    "            type_of_data = type_data\n",
    "            final_X = X_selected\n",
    "            final_y = y\n",
    "            final_x_transformer = selector\n",
    "\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_models(df_current, X, y, dataset_type, type_data, pca_transformer ):\n",
    "    global final_model, final_model_cv_score, final_model_score, final_scoring_type, final_dataset, type_of_data, base_score, final_X, final_y, final_x_transformer\n",
    "\n",
    "    scoring_method = 'precision'  # Change to 'f1_macro', 'f1_micro', or 'f1_weighted' for multi-class\n",
    "\n",
    "    if len(df_current) < 100:\n",
    "        cv_strategy = LeaveOneOut()\n",
    "    else:\n",
    "        cv_strategy = LeaveOneOut()#KFold(n_splits=15, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        cv_scores = np.mean(cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring_method, n_jobs=-1))\n",
    "        print(name,cv_scores)\n",
    "        if name == 'Bayesian' and cv_scores == final_model_cv_score:\n",
    "            cv_scores += 0.001\n",
    "\n",
    "        if cv_scores > final_model_cv_score:\n",
    "            final_model = name\n",
    "            final_model_cv_score = cv_scores\n",
    "            final_model_score = cv_scores - base_score\n",
    "            final_scoring_type = 'loo' if len(df_current) < 100 else 'cv'\n",
    "            final_dataset = dataset_type\n",
    "            type_of_data = type_data\n",
    "            final_X = X\n",
    "            final_y = y\n",
    "            final_x_transformer = pca_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5b86d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for neural network\n",
    "def create_neural_network(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(),\n",
    "#     \"SVM\": SVC(probability=True),\n",
    "#     \"Naive Bayes\": GaussianNB(),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_jobs=-1),\n",
    "#     \"Gradient Boosting Machines\": GradientBoostingClassifier(),\n",
    "#     \"KNN\": KNeighborsClassifier(),\n",
    "#     \"AdaBoost\": AdaBoostClassifier(),\n",
    "#     \"XGBoost\": XGBClassifier(n_jobs=-1),\n",
    "#     \"CatBoost\": CatBoostClassifier(thread_count=-1),\n",
    "#     \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "#     \"SGD Classifier\": SGDClassifier(loss='log', n_jobs=-1)\n",
    "#     \"Neural Network (TensorFlow)\": create_neural_network  # Assuming input_shape is defined elsewhere\n",
    "}\n",
    "\n",
    "# Note: The neural network model requires additional setup like defining input_shape based on your specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e404f63e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_score = sum(df['PtsThreshold'])/ len(df) \n",
    "final_model = ''\n",
    "final_model_cv_score = 0\n",
    "final_model_score = 0\n",
    "final_scoring_type = 'career'\n",
    "final_dataset = -999\n",
    "type_of_data = 'noraml_normal'\n",
    "final_x_transformer = None\n",
    "\n",
    "\n",
    "final_X = None\n",
    "final_y = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a10ebda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6057906458797327"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['PtsThreshold'])/ len(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b86229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models with dimensionality reduction:\n",
      "Random Forest 0.65\n",
      "Quadratic Discriminant Analysis 0.6666666666666666\n",
      "\n",
      "Evaluating models with dimensionality reduction only n comp:\n",
      "Random Forest 0.6166666666666667\n",
      "Quadratic Discriminant Analysis 0.6666666666666666\n",
      "\n",
      "Evaluating models with top 5% feature selection:\n",
      "Random Forest 0.5833333333333334\n",
      "Quadratic Discriminant Analysis 0.6666666666666666\n",
      "\n",
      "Evaluating models with top 10% feature selection:\n",
      "Random Forest 0.5833333333333334\n",
      "Quadratic Discriminant Analysis 0.25\n"
     ]
    }
   ],
   "source": [
    "base_score = sum(df_last_60['PtsThreshold'])/ len(df_last_60) \n",
    "\n",
    "max_feat_5_pct = round(len(df)*.05)\n",
    "max_feat_10_pct = round(len(df)*.10)\n",
    "\n",
    "df_last_60.fillna(df_last_60.mean(), inplace=True)\n",
    "\n",
    "# Separating features and target\n",
    "X = df_last_60.drop('PtsThreshold', axis=1)\n",
    "y = df_last_60['PtsThreshold']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=0.95) # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca_n_com = PCA(n_components=max_feat_5_pct) # Adjust the number of components as needed\n",
    "X_pca_n_comp = pca_n_com.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# # Evaluating models without dimensionality reduction\n",
    "# print(\"Evaluating models without dimensionality reduction:\")\n",
    "# evaluate_models(df_last_60,X_scaled, y,'60','normal_normal')\n",
    "\n",
    "# Evaluating models with dimensionality reduction\n",
    "print(\"\\nEvaluating models with dimensionality reduction:\")\n",
    "evaluate_models(df_last_60,X_pca, y,'60','pca_95', pca_transformer = pca)\n",
    "\n",
    "# Evaluating models with dimensionality reduction only n comp\n",
    "print(\"\\nEvaluating models with dimensionality reduction only n comp:\")\n",
    "evaluate_models(df_last_60,X_pca_n_comp, y,'60','pca_n',pca_transformer = pca_n_com)\n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 5% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_last_60,X_scaled, y, max_feat_5_pct,'60','selection_5')  \n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 10% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_last_60,X_scaled, y, max_feat_10_pct,'60','selection_10')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de4d9384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_last_60['PtsThreshold'])/ len(df_last_60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44dd24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models with dimensionality reduction: pca_95 PCA(n_components=0.95)\n",
      "Quadratic Discriminant Analysis 0.725\n",
      "\n",
      "Evaluating models with dimensionality reduction only n comp: PCA(n_components=4)\n",
      "Quadratic Discriminant Analysis 0.6\n",
      "\n",
      "Evaluating models with top 5% feature selection:\n",
      "Quadratic Discriminant Analysis 0.7\n",
      "\n",
      "Evaluating models with top 10% feature selection:\n",
      "Quadratic Discriminant Analysis 0.675\n"
     ]
    }
   ],
   "source": [
    "df_season.fillna(df_season.mean(), inplace=True)\n",
    "base_score = sum(df_season['PtsThreshold'])/ len(df_season) \n",
    "\n",
    "# Separating features and target\n",
    "X = df_season.drop('PtsThreshold', axis=1)\n",
    "y = df_season['PtsThreshold']\n",
    "\n",
    "max_feat_5_pct = round(len(df_season)*.05)\n",
    "max_feat_10_pct = round(len(df_season)*.10)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=0.95) # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca_n_com = PCA(n_components=max_feat_10_pct) # Adjust the number of components as needed\n",
    "X_pca_n_comp = pca_n_com.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# # Evaluating models without dimensionality reduction\n",
    "# print(\"Evaluating models without dimensionality reduction:\")\n",
    "# evaluate_models(df_season,X_scaled, y, 'season','normal_normal')\n",
    "\n",
    "# Evaluating models with dimensionality reduction\n",
    "print(\"\\nEvaluating models with dimensionality reduction:\",'pca_95', pca)\n",
    "evaluate_models(df_season,X_pca, y,'season','pca_95',pca_transformer = pca)\n",
    "\n",
    "# Evaluating models with dimensionality reduction only n comp\n",
    "print(\"\\nEvaluating models with dimensionality reduction only n comp:\",pca_n_com)\n",
    "evaluate_models(df_season,X_pca_n_comp, y,'season','pca_n',pca_transformer = pca_n_com)\n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 5% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_season,X_scaled, y, max_feat_5_pct,'season','selection_5')  \n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 10% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_season,X_scaled, y, max_feat_10_pct,'season','selection_10')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a45285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_season['PtsThreshold'])/ len(df_season) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd0a227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models with dimensionality reduction:\n",
      "Random Forest 0.7439024390243902\n",
      "Quadratic Discriminant Analysis 0.7439024390243902\n",
      "\n",
      "Evaluating models with dimensionality reduction only n comp:\n",
      "Random Forest 0.6585365853658537\n",
      "Quadratic Discriminant Analysis 0.6341463414634146\n",
      "\n",
      "Evaluating models with top 5% feature selection:\n",
      "Random Forest 0.6219512195121951\n",
      "Quadratic Discriminant Analysis 0.6829268292682927\n",
      "\n",
      "Evaluating models with top 10% feature selection:\n",
      "Random Forest 0.7073170731707317\n",
      "Quadratic Discriminant Analysis 0.6707317073170732\n"
     ]
    }
   ],
   "source": [
    "base_score = sum(df_season_buffer['PtsThreshold'])/ len(df_season_buffer) \n",
    "\n",
    "max_feat_5_pct = round(len(df_season_buffer)*.05)\n",
    "max_feat_10_pct = round(len(df_season_buffer)*.10)\n",
    "\n",
    "df_season_buffer.fillna(df_season_buffer.mean(), inplace=True)\n",
    "\n",
    "# Separating features and target\n",
    "X = df_season_buffer.drop('PtsThreshold', axis=1)\n",
    "y = df_season_buffer['PtsThreshold']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=0.95) # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca_n_com = PCA(n_components=max_feat_5_pct) # Adjust the number of components as needed\n",
    "X_pca_n_comp = pca_n_com.fit_transform(X_scaled)\n",
    "\n",
    "# Evaluating models with dimensionality reduction\n",
    "print(\"\\nEvaluating models with dimensionality reduction:\")\n",
    "evaluate_models(df_season_buffer,X_pca, y,'buffer','pca_95', pca_transformer = pca)\n",
    "\n",
    "# Evaluating models with dimensionality reduction only n comp\n",
    "print(\"\\nEvaluating models with dimensionality reduction only n comp:\")\n",
    "evaluate_models(df_season_buffer,X_pca_n_comp, y,'buffer','pca_n',pca_transformer = pca_n_com)\n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 5% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_season_buffer,X_scaled, y, max_feat_5_pct,'buffer','selection_5')  \n",
    "\n",
    "# Example of how to use the modified function\n",
    "print(\"\\nEvaluating models with top 10% feature selection:\")\n",
    "evaluate_models_with_feature_selection(df_season_buffer,X_scaled, y, max_feat_10_pct,'buffer','selection_10')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cb800e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439024390243902"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_season_buffer['PtsThreshold'])/ len(df_season_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "137ad0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.7439024390243902\n",
      "0.0\n",
      "loo\n",
      "buffer\n",
      "pca_95\n",
      "PCA(n_components=0.95)\n"
     ]
    }
   ],
   "source": [
    "print(final_model)\n",
    "print(final_model_cv_score)\n",
    "print(final_model_score )\n",
    "print(final_scoring_type) \n",
    "print(final_dataset)\n",
    "print(type_of_data)\n",
    "print(final_x_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58647df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Discriminant Analysis\n",
      "100%|| 250/250 [00:41<00:00,  6.01trial/s, best loss: -0.7439024390243902]\n",
      "Best F1 Score: 0.7439024390243902\n"
     ]
    }
   ],
   "source": [
    "if final_model == 'Logistic Regression':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_lr(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    lr = LogisticRegression(**params).fit(final_X, final_y)\n",
    "    \n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = lr.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = lr.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = lr.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    prediction = final_prediction[0]\n",
    "elif final_model == 'Bayesian':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_nb(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    nb = GaussianNB(**params).fit(final_X, final_y)\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = nb.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "\n",
    "    # Cross-validation for model accuracy\n",
    "    scores = cross_val_score(nb, final_X, final_y, cv=loo, n_jobs=-1)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = nb.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = nb.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prediction = final_prediction[0]\n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    \n",
    "elif final_model == 'Decision Tree':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_dt(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    dt = DecisionTreeClassifier(**params).fit(final_X, final_y)\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = dt.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = dt.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = dt.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prediction = final_prediction[0]\n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    \n",
    "elif final_model == 'Random Forest':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_rf(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    rf = RandomForestClassifier(**params, n_jobs=-1).fit(final_X, final_y)\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = rf.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = rf.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = rf.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prediction = final_prediction[0]\n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    \n",
    "elif final_model == 'XGBoost':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_xgb(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    xgb = XGBClassifier(**params).fit(final_X, final_y)\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = xgb.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = xgb.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = xgb.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prediction = final_prediction[0]\n",
    "    \n",
    "elif final_model == 'LightGBM':\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    final_model_cv_score=.01\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_lgbm(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    lgbm = LGBMClassifier(**params, verbose=0).fit(final_X, final_y)\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = lgbm.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = lgbm.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = lgbm.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    prediction = final_prediction[0]\n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    \n",
    "elif final_model == 'SVM':   \n",
    "    best_score =0 \n",
    "    tracker = 0\n",
    "    while best_score <final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_svc(final_X,final_y,randint(0,20000))\n",
    "        if tracker ==2:\n",
    "            break\n",
    "        tracker+=1\n",
    "    svm = SVC(**params, probability=True, verbose=False).fit(final_X, final_y)\n",
    "    prob_positive_class = svm.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold\n",
    "    best_threshold = find_best_threshold(prob_positive_class, final_y)\n",
    "    \n",
    "    # Prediction of probabilities\n",
    "    # Assuming final_X_test is your test set features\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class = svm.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class = svm.predict_proba(pred)[:, 1]\n",
    "\n",
    "    prediction = int(prob_positive_class[0]>best_threshold)\n",
    "    \n",
    "elif final_model == 'KNN':\n",
    "    print('KNN BABY')\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_knn(final_X, final_y, randint(0, 20000))\n",
    "\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    knn = KNeighborsClassifier(**params).fit(final_X, final_y)\n",
    "    \n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        prediction = knn.predict(final_x_transformer.transform(pred))\n",
    "\n",
    "elif final_model == 'SGD Classifier':\n",
    "    print('SGD')\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_sgd(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    # Initialize the SGD Classifier with log loss for probability estimation\n",
    "    sgd = SGDClassifier(loss='log', **params).fit(final_X, final_y)\n",
    "    \n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = sgd.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = sgd.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = sgd.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    prediction = final_prediction[0]\n",
    "elif final_model == 'Quadratic Discriminant Analysis':\n",
    "    print('Quadratic Discriminant Analysis')\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_qda(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    # Create and fit the Quadratic Discriminant Analysis model with the best parameters\n",
    "    qda = QuadraticDiscriminantAnalysis(**params).fit(final_X, final_y)\n",
    "    \n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = qda.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using the training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = qda.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = qda.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make the final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prob_positive_class_test=prob_positive_class_test[0]\n",
    "    \n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n",
    "    prediction = final_prediction[0]\n",
    "else:\n",
    "    best_score = 0\n",
    "    tracker = 0\n",
    "    while best_score < final_model_cv_score:\n",
    "        params, best_score = perform_hyper_param_tuning_bayes_nb(final_X, final_y, randint(0, 20000))\n",
    "        if tracker == 2:\n",
    "            break\n",
    "        tracker += 1\n",
    "\n",
    "    nb = GaussianNB(**params).fit(final_X, final_y)\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # Get predicted probabilities for the positive class on the training set\n",
    "    prob_positive_class_train = nb.predict_proba(final_X)[:, 1]\n",
    "\n",
    "    # Find the best threshold using training set\n",
    "    best_threshold = find_best_threshold(prob_positive_class_train, final_y)\n",
    "\n",
    "    # Cross-validation for model accuracy\n",
    "    scores = cross_val_score(nb, final_X, final_y, cv=loo, n_jobs=-1)\n",
    "    \n",
    "    # Prediction of probabilities on the test set\n",
    "    if 'SelectFromModel' in str(final_x_transformer) or 'PCA' in str(final_x_transformer):\n",
    "        transformed_X_test = final_x_transformer.transform(pred)\n",
    "        prob_positive_class_test = nb.predict_proba(transformed_X_test)[:, 1]\n",
    "    else:\n",
    "        prob_positive_class_test = nb.predict_proba(pred)[:, 1]\n",
    "\n",
    "    # Apply the custom threshold to make final binary prediction\n",
    "    final_prediction = np.where(prob_positive_class_test > best_threshold, 1, 0)\n",
    "    \n",
    "    prediction = final_prediction[0]\n",
    "    # Assuming you are interested in the first prediction if 'pred' contains multiple samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "699ae72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((qda.predict(final_X) ==final_y).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_positive_class_test\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ff796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyper_param_tuning_bayes_lr(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Unpack the parameters\n",
    "        params = {\n",
    "            'C': params['C'],\n",
    "            'penalty': ['l2', 'none'][params['penalty']],\n",
    "            'solver': ['lbfgs', 'saga', 'newton-cg'][params['solver']],\n",
    "            'class_weight': ['balanced', None][params['class_weight']],\n",
    "            'fit_intercept': [True, False][params['fit_intercept']],\n",
    "            'tol': params['tol'],\n",
    "            'max_iter': params['max_iter']\n",
    "        }\n",
    "\n",
    "        # Create a Logistic Regression model with given parameters\n",
    "        lr = LogisticRegression(**params, random_state=seed)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(lr, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the enhanced parameter space for Logistic Regression\n",
    "    search_space = {\n",
    "        'C': hp.loguniform('C', np.log(1e-6), np.log(1e+6)),\n",
    "        'penalty': hp.choice('penalty', range(2)),\n",
    "        'solver': hp.choice('solver', range(3)),\n",
    "        'class_weight': hp.choice('class_weight', range(2)),\n",
    "        'fit_intercept': hp.choice('fit_intercept', range(2)),\n",
    "        'tol': hp.loguniform('tol', np.log(1e-6), np.log(1e-3)),\n",
    "        'max_iter': hp.choice('max_iter', range(100, 1001))  # Range of integers\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=200,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Map indices to actual parameters\n",
    "    best_params_mapped = {\n",
    "        'C': best_params['C'],\n",
    "        'penalty': ['l2', 'none'][best_params['penalty']],\n",
    "        'solver': ['lbfgs', 'saga', 'newton-cg'][best_params['solver']],\n",
    "        'class_weight': ['balanced', None][best_params['class_weight']],\n",
    "        'fit_intercept': [True, False][best_params['fit_intercept']],\n",
    "        'tol': best_params['tol'],\n",
    "        'max_iter': best_params['max_iter']\n",
    "    }\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "    return best_params_mapped, best_score\n",
    "\n",
    "\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_nb(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Create a GaussianNB model with given parameters\n",
    "        gnb = GaussianNB(**params)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(gnb, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for GaussianNB\n",
    "    search_space = {\n",
    "        'var_smoothing': hp.loguniform('var_smoothing', np.log(1e-10), np.log(1e-1))\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=1000,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_dt(final_X, final_y,seed):\n",
    "    def objective(params):\n",
    "        # Create a DecisionTreeClassifier model with given parameters\n",
    "        dt = DecisionTreeClassifier(**params, random_state=seed)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(dt, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for DecisionTreeClassifier\n",
    "    search_space = {\n",
    "        'max_depth': hp.choice('max_depth', range(1, 15)),\n",
    "        'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5),\n",
    "        'criterion': hp.choice('criterion', ['gini', 'entropy'])\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=100,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "    maps = ['gini', 'entropy']\n",
    "    best_params['criterion'] = maps[best_params['criterion']]\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_rf(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Create a RandomForestClassifier model with given parameters\n",
    "        rf = RandomForestClassifier(**params, random_state=seed, n_jobs=-1)\n",
    "        # Using Leave-One-Out cross-validation\n",
    "        cv = StratifiedKFold(n_splits=round(len(final_X)/2), random_state = seed, shuffle  = True)\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean ROC AUC score\n",
    "        roc_auc_scores = cross_val_score(rf, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_roc_auc = np.mean(roc_auc_scores)\n",
    "        \n",
    "        # Hyperopt tries to minimize the objective, so return the negative ROC AUC score\n",
    "        return {'loss': -mean_roc_auc, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    # Define the parameter space for RandomForestClassifier\n",
    "    search_space = {\n",
    "        'n_estimators': hp.choice('n_estimators', range(10, 1001)),\n",
    "        'max_depth': hp.choice('max_depth', range(1, 15)),\n",
    "        'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None])\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=100,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_xgb(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Create an XGBClassifier model with given parameters\n",
    "        xgb = XGBClassifier(**params)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(xgb, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for XGBClassifier\n",
    "    search_space = {\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 1001)),\n",
    "        'max_depth': hp.choice('max_depth', range(3, 15)),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': hp.uniform('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': hp.uniform('gamma', 0.0, 5.0)\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=100,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_svc(final_X, final_y,seed):\n",
    "    def objective(params):\n",
    "        # Create an SVC model with given parameters\n",
    "        svc = SVC(**params, random_state=seed)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = StratifiedKFold(n_splits=15,random_state = seed, shuffle  = True)\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(svc, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for SVC\n",
    "    search_space = {\n",
    "        'C': hp.loguniform('C', np.log(1e-3), np.log(1e3)),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "        'gamma': hp.loguniform('gamma', np.log(1e-4), np.log(1e1))\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=1000,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "    maps = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "    best_params['kernel'] = maps[best_params['kernel']]\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "def perform_hyper_param_tuning_knn(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Create a KNeighborsClassifier model with given parameters\n",
    "        knn = KNeighborsClassifier(**params)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(knn, final_X, final_y, cv=cv, scoring='precision',  n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for KNN\n",
    "    search_space = {\n",
    "        'n_neighbors': hp.choice('n_neighbors', range(1, 30)),  # Number of neighbors\n",
    "        'metric': hp.choice('metric', ['euclidean', 'manhattan', 'minkowski']),  # Distance metric\n",
    "        'weights': hp.choice('weights', ['uniform', 'distance'])  # Weighting function\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=250,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "    best_params['metric'] = ['euclidean', 'manhattan', 'minkowski'][best_params['metric']]\n",
    "    best_params['weights'] = ['uniform', 'distance'][best_params['weights']]\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_sgd(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Unpack the parameters\n",
    "        params = {\n",
    "            'alpha': params['alpha'],\n",
    "            'penalty': ['l2', 'l1', 'elasticnet'][params['penalty']],\n",
    "            'fit_intercept': [True, False][params['fit_intercept']],\n",
    "            'tol': params['tol'],\n",
    "            'max_iter': params['max_iter']\n",
    "        }\n",
    "\n",
    "        # Create an SGD Classifier model with given parameters\n",
    "        sgd = SGDClassifier(**params, random_state=seed)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(sgd, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the enhanced parameter space for SGD Classifier\n",
    "    search_space = {\n",
    "        'alpha': hp.loguniform('alpha', np.log(1e-6), np.log(1e-2)),\n",
    "        'penalty': hp.choice('penalty', range(3)),\n",
    "        'fit_intercept': hp.choice('fit_intercept', range(2)),\n",
    "        'tol': hp.loguniform('tol', np.log(1e-6), np.log(1e-3)),\n",
    "        'max_iter': hp.choice('max_iter', range(100, 1001))  # Range of integers\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=200,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Map indices to actual parameters\n",
    "    best_params_mapped = {\n",
    "        'alpha': best_params['alpha'],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'][best_params['penalty']],\n",
    "        'fit_intercept': [True, False][best_params['fit_intercept']],\n",
    "        'tol': best_params['tol'],\n",
    "        'max_iter': best_params['max_iter']\n",
    "    }\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "    return best_params_mapped, best_score\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_qda(final_X, final_y, seed):\n",
    "    def objective(params):\n",
    "        # Create a QDA model with given parameters\n",
    "        qda = QuadraticDiscriminantAnalysis(reg_param=params['reg_param'])\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = LeaveOneOut()\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(qda, final_X, final_y, cv=cv, scoring='precision', n_jobs=-1)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for QDA\n",
    "    search_space = {\n",
    "        'reg_param': hp.uniform('reg_param', 0, 3)  # Regularization parameter\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=250,  \n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(seed))\n",
    "\n",
    "    # Extracting the best score and parameters\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "    best_reg_param = trials.argmin['reg_param']\n",
    "\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "    return {'reg_param': best_reg_param}, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c0c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_hyper_param_tuning_bayes(x,y):\n",
    "    # Define the parameter space for GaussianNB\n",
    "    # GaussianNB doesn't have many hyperparameters, but we can tune 'var_smoothing'\n",
    "    search_space = {\n",
    "        'var_smoothing': Real(1e-9, 1e-3, prior='log-uniform')\n",
    "    }\n",
    "\n",
    "    # Create a GaussianNB model instance\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    # Setup Bayesian optimization with Leave-One-Out cross-validation\n",
    "    bayes_cv_tuner = BayesSearchCV(\n",
    "        estimator=gnb,\n",
    "        search_spaces=search_space,\n",
    "        scoring='accuracy',\n",
    "        cv=LeaveOneOut(),  # Using Leave-One-Out cross-validation\n",
    "        n_jobs=-1,  # Use all cores\n",
    "        n_iter=32,  # Number of parameter settings sampled\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    bayes_cv_tuner.fit(X, y)\n",
    "\n",
    "    # Best parameters and score\n",
    "    best_params = bayes_cv_tuner.best_params_\n",
    "    best_score = bayes_cv_tuner.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Score: {best_score}\")\n",
    "\n",
    "\n",
    "def perform_hyper_param_tuning_bayes_nb(final_X, final_y):\n",
    "    def objective(params):\n",
    "        # Create a GaussianNB model with given parameters\n",
    "        gnb = GaussianNB(**params)\n",
    "\n",
    "        # Using Stratified K-Fold cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "        # Evaluate the model using cross-validation and return the mean F1 score\n",
    "        f1_scores = cross_val_score(gnb, final_X, final_y, cv=cv, scoring='f1')\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Hyperopt tries to minimize the objective, so return the negative F1 score\n",
    "        return {'loss': -mean_f1, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the parameter space for GaussianNB\n",
    "    search_space = {\n",
    "        'var_smoothing': hp.loguniform('var_smoothing', np.log(1e-9), np.log(1e-1))\n",
    "    }\n",
    "\n",
    "    # Perform the hyperparameter tuning\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals=100,  # Adjust the number of evaluations\n",
    "                       trials=trials)\n",
    "\n",
    "    # Extracting the best score\n",
    "    best_score = -min([trial['result']['loss'] for trial in trials.trials])\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "# Sample usage (replace final_X, final_y with your data)\n",
    "# final_X, final_y = ...\n",
    "# perform_hyper_param_tuning_bayes_nb(final_X, final_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0810ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_top_n_features(X_scaled, y,max_feat_10_pct)\n",
    "perform_hyper_param_tuning_bayes(final_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427045fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "model = MLPClassifier()\n",
    "\n",
    "# Define the parameters for tuning\n",
    "# Adjust these based on your needs and computational constraints\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 300, 500]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo)\n",
    "\n",
    "# Fit the model to the data\n",
    "# X and y should be your data\n",
    "grid_search.fit(X_pca, y)\n",
    "\n",
    "# Get the best parameters and the corresponding score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \", best_parameters)\n",
    "print(\"Best Cross-Validation Score: {:.4f}\".format(best_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(alpha = 0.0001, hidden_layer_sizes = (50,), learning_rate_init= 0.1, max_iter= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(pca.transform(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf42dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_hyper_param_tuning(X_pca,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the parameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Create Leave-One-Out cross-validation object\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Create a GridSearchCV object with Leave-One-Out cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo)\n",
    "\n",
    "# Fit the model to the data\n",
    "# X_pca and y should be your training data\n",
    "grid_search.fit(X_pca, y)\n",
    "\n",
    "# Get the best parameters and the corresponding score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \", best_parameters)\n",
    "print(\"Best Cross-Validation Score: {:.4f}\".format(best_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfb983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_season['PtsThreshold'])/ len(df_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(df_season)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_pca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the data\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.plot(data['GAME_DATE'], data['PTS'], label='PTS', marker='o')\n",
    "# # plt.plot(data['GAME_DATE'], data['PTSLast60'], label='PTSLast60')\n",
    "# # plt.plot(data['GAME_DATE'], data['PTSLast10'], label='PTSLast10')\n",
    "# plt.xlabel('Game Date')\n",
    "# plt.ylabel('Points')\n",
    "# plt.title('Game Date vs Points')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
